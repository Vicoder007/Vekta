#!/usr/bin/env python3
"""
Test de Configuration Mac M3 pour Vekta V2
V√©rifie que tout est correctement configur√© pour votre Mac M3
"""

import subprocess
import requests
import time
import sys
import os

def test_ollama_installation():
    """Test de l'installation Ollama"""
    print("üîç Test d'installation Ollama...")
    
    try:
        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print(f"‚úÖ Ollama install√©: {result.stdout.strip()}")
            return True
        else:
            print("‚ùå Ollama non install√©")
            return False
    except FileNotFoundError:
        print("‚ùå Ollama non trouv√© dans PATH")
        return False

def test_ollama_service():
    """Test du service Ollama"""
    print("üîç Test du service Ollama...")
    
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Service Ollama actif")
            return True
        else:
            print(f"‚ùå Service Ollama inactif (code: {response.status_code})")
            return False
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Service Ollama non accessible: {e}")
        return False

def test_model_availability():
    """Test de la disponibilit√© du mod√®le Mac M3"""
    print("üîç Test du mod√®le llama3.2:3b...")
    
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_names = [model.get("name", "") for model in models]
            
            for name in model_names:
                if "llama3.2:3b" in name:
                    print(f"‚úÖ Mod√®le Mac M3 disponible: {name}")
                    return True
            
            print("‚ùå Mod√®le llama3.2:3b non trouv√©")
            print("üí° Mod√®les disponibles:")
            for name in model_names:
                print(f"   - {name}")
            return False
    except Exception as e:
        print(f"‚ùå Impossible de v√©rifier les mod√®les: {e}")
        return False

def test_model_performance():
    """Test de performance du mod√®le"""
    print("üîç Test de performance du mod√®le...")
    
    test_prompt = "G√©n√®re un √©chauffement cycliste de 10 minutes."
    
    try:
        start_time = time.time()
        
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "llama3.2:3b",
                "prompt": test_prompt,
                "stream": False,
                "options": {
                    "num_predict": 100,
                    "temperature": 0.1
                }
            },
            timeout=30
        )
        
        end_time = time.time()
        
        if response.status_code == 200:
            response_time = end_time - start_time
            result = response.json().get("response", "")
            
            print(f"‚úÖ Test de performance r√©ussi")
            print(f"   ‚è±Ô∏è  Temps de r√©ponse: {response_time:.2f}s")
            print(f"   üìù R√©ponse (extrait): {result[:100]}...")
            
            if response_time < 10:
                print("   üöÄ Performance excellente pour Mac M3")
            elif response_time < 20:
                print("   üëç Performance correcte pour Mac M3")
            else:
                print("   ‚ö†Ô∏è  Performance lente - v√©rifiez la m√©moire disponible")
            
            return True
        else:
            print(f"‚ùå Test √©chou√© (code: {response.status_code})")
            return False
            
    except Exception as e:
        print(f"‚ùå Test de performance √©chou√©: {e}")
        return False

def test_system_resources():
    """Test des ressources syst√®me"""
    print("üîç Test des ressources syst√®me...")
    
    try:
        import psutil
        
        # M√©moire
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        total_gb = memory.total / (1024**3)
        
        print(f"   üíæ M√©moire: {available_gb:.1f}GB libre / {total_gb:.1f}GB total")
        
        if available_gb >= 8:
            print("   ‚úÖ M√©moire excellente pour llama3.2:3b")
        elif available_gb >= 4:
            print("   ‚úÖ M√©moire suffisante pour llama3.2:3b")
        else:
            print("   ‚ö†Ô∏è  M√©moire faible - performances r√©duites possibles")
        
        # CPU
        cpu_count = psutil.cpu_count()
        cpu_percent = psutil.cpu_percent(interval=1)
        
        print(f"   üî¢ CPU: {cpu_count} c≈ìurs, utilisation: {cpu_percent}%")
        
        if cpu_count >= 8:
            print("   ‚úÖ CPU excellent pour Mac M3")
        elif cpu_count >= 4:
            print("   ‚úÖ CPU suffisant")
        else:
            print("   ‚ö†Ô∏è  CPU limit√©")
        
        return True
        
    except ImportError:
        print("   ‚ö†Ô∏è  psutil non install√© - impossible de v√©rifier les ressources")
        return True
    except Exception as e:
        print(f"   ‚ùå Erreur lors de la v√©rification: {e}")
        return True

def test_dependencies():
    """Test des d√©pendances Python"""
    print("üîç Test des d√©pendances Python...")
    
    required_packages = [
        'streamlit',
        'requests',
        'plotly',
        'pandas',
        'psutil'
    ]
    
    missing = []
    for package in required_packages:
        try:
            __import__(package)
            print(f"   ‚úÖ {package}")
        except ImportError:
            print(f"   ‚ùå {package} - MANQUANT")
            missing.append(package)
    
    if missing:
        print(f"üí° Installez les packages manquants: pip install {' '.join(missing)}")
        return False
    
    return True

def main():
    """Ex√©cute tous les tests"""
    print("üß™ TEST DE CONFIGURATION MAC M3 POUR VEKTA V2")
    print("=" * 50)
    
    tests = [
        ("Installation Ollama", test_ollama_installation),
        ("Service Ollama", test_ollama_service),
        ("Mod√®le Mac M3", test_model_availability),
        ("Performance", test_model_performance),
        ("Ressources Syst√®me", test_system_resources),
        ("D√©pendances Python", test_dependencies)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        print(f"\nüìã {test_name}")
        print("-" * 30)
        
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå Erreur inattendue: {e}")
            results.append((test_name, False))
    
    # R√©sum√©
    print("\n" + "=" * 50)
    print("üìä R√âSUM√â DES TESTS")
    print("=" * 50)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} - {test_name}")
        if result:
            passed += 1
    
    print(f"\nüéØ Score: {passed}/{total} tests pass√©s")
    
    if passed == total:
        print("\nüéâ Configuration Mac M3 parfaite !")
        print("üö¥‚Äç‚ôÇÔ∏è Vekta V2 est pr√™t √† l'emploi")
        print("\nüí° Lancez l'application avec: python3 launch_vekta_v2.py")
    elif passed >= total - 1:
        print("\nüëç Configuration Mac M3 tr√®s bonne")
        print("‚ö†Ô∏è  Un test mineur a √©chou√©, mais Vekta V2 devrait fonctionner")
    else:
        print("\n‚ö†Ô∏è  Configuration incompl√®te")
        print("üîß Veuillez corriger les erreurs avant d'utiliser Vekta V2")
        
        if not any(result for name, result in results if "Ollama" in name):
            print("\nüí° Solution recommand√©e:")
            print("   ./install_ollama_mac.sh")

if __name__ == "__main__":
    main() 