#!/usr/bin/env python3
"""
Lanceur Vekta V2 - Intelligence Architecture avec Auto-D√©marrage Ollama
Architecture LLM + Validation Stricte + Auto-Configuration Ollama
"""

import subprocess
import sys
import os
import time
import signal
import threading
import requests
from pathlib import Path

# Variables globales pour les processus
ollama_process = None
streamlit_process = None

def check_ollama_installed():
    """V√©rifie si Ollama est install√©"""
    try:
        result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ Ollama est install√©")
            return True
    except FileNotFoundError:
        pass
    
    print("‚ùå Ollama n'est pas install√©")
    print("üì• Veuillez installer Ollama depuis : https://ollama.ai")
    return False

def check_ollama_running():
    """V√©rifie si Ollama est d√©j√† en cours d'ex√©cution"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Ollama est d√©j√† en cours d'ex√©cution")
            return True
    except requests.exceptions.RequestException:
        pass
    
    print("üîÑ Ollama n'est pas en cours d'ex√©cution")
    return False

def start_ollama():
    """D√©marre Ollama en arri√®re-plan"""
    global ollama_process
    
    print("üöÄ D√©marrage d'Ollama...")
    
    try:
        # D√©marrer Ollama en arri√®re-plan avec optimisations Mac M3
        env = os.environ.copy()
        env['OLLAMA_MODELS'] = os.path.expanduser('~/.ollama/models')
        env['OLLAMA_HOST'] = 'localhost:11434'
        env['OLLAMA_KEEP_ALIVE'] = '5m'  # Garde le mod√®le en m√©moire 5 minutes
        env['OLLAMA_NUM_PARALLEL'] = '1'  # Une seule requ√™te √† la fois pour √©conomiser la m√©moire
        
        ollama_process = subprocess.Popen(
            ['ollama', 'serve'],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            env=env
        )
        
        # Attendre que Ollama soit pr√™t (max 30 secondes)
        for i in range(30):
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=2)
                if response.status_code == 200:
                    print("‚úÖ Ollama d√©marr√© avec succ√®s")
                    return True
            except requests.exceptions.RequestException:
                pass
            
            print(f"‚è≥ Attente d'Ollama... ({i+1}/30)")
            time.sleep(1)
        
        print("‚ùå Timeout: Ollama n'a pas pu d√©marrer")
        return False
        
    except Exception as e:
        print(f"‚ùå Erreur lors du d√©marrage d'Ollama: {e}")
        return False

def pull_optimal_model():
    """T√©l√©charge le mod√®le optimal pour Mac M3"""
    optimal_model = "llama3.2:3b"
    
    print(f"üß† V√©rification du mod√®le optimal: {optimal_model}")
    
    try:
        # V√©rifier si le mod√®le est d√©j√† disponible
        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_names = [model.get("name", "") for model in models]
            
            if any(optimal_model in name for name in model_names):
                print(f"‚úÖ Mod√®le {optimal_model} d√©j√† disponible")
                return True
        
        # T√©l√©charger le mod√®le
        print(f"üì• T√©l√©chargement du mod√®le {optimal_model}...")
        print("üí° Ce mod√®le est optimis√© pour Mac M3 (3B param√®tres)")
        print("‚è≥ T√©l√©chargement en cours... (peut prendre quelques minutes)")
        
        pull_result = subprocess.run(
            ['ollama', 'pull', optimal_model],
            capture_output=True,
            text=True,
            timeout=600  # 10 minutes max
        )
        
        if pull_result.returncode == 0:
            print(f"‚úÖ Mod√®le {optimal_model} t√©l√©charg√© avec succ√®s")
            return True
        else:
            print(f"‚ùå Erreur lors du t√©l√©chargement: {pull_result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print("‚ùå Timeout lors du t√©l√©chargement")
        return False
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return False

def setup_ollama():
    """Configuration d'Ollama avec mod√®le optimal pour Mac M3"""
    print("üß† === CONFIGURATION OLLAMA POUR MAC M3 ===")
    
    # 1. V√©rifier l'installation
    if not check_ollama_installed():
        return False
    
    # 2. V√©rifier si d√©j√† en cours
    if not check_ollama_running():
        # 3. D√©marrer Ollama
        if not start_ollama():
            return False
    
    # 4. T√©l√©charger le mod√®le optimal
    if not pull_optimal_model():
        print("‚ö†Ô∏è  Impossible de t√©l√©charger le mod√®le optimal")
        print("üí° L'application fonctionnera, mais le mod√®le sera t√©l√©charg√© au premier usage")
    
    print("‚úÖ Ollama configur√© et pr√™t avec mod√®le Mac M3")
    return True

def check_dependencies():
    """V√©rification des d√©pendances V2"""
    print("üîç V√©rification des d√©pendances Python...")
    
    required_packages = [
        'streamlit',
        'requests', 
        'plotly',
        'pandas'
    ]
    
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
            print(f"  ‚úÖ {package}")
        except ImportError:
            print(f"  ‚ùå {package} - MANQUANT")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\n‚ö†Ô∏è  Packages manquants: {', '.join(missing_packages)}")
        print("üì¶ Installation avec: pip install -r requirements.txt")
        
        install = input("Installer maintenant? (y/N): ").lower().strip()
        if install == 'y':
            subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'])
        else:
            sys.exit(1)
    
    print("‚úÖ Toutes les d√©pendances Python sont install√©es")

def launch_streamlit_v2():
    """Lance l'interface Streamlit V2"""
    global streamlit_process
    
    print("üöÄ Lancement de l'interface Vekta V2...")
    
    # V√©rifier que le fichier existe
    app_file = Path('frontend/vekta_app_simple.py')
    if not app_file.exists():
        print(f"‚ùå Fichier non trouv√©: {app_file}")
        sys.exit(1)
    
    # Lancer Streamlit
    cmd = [
        sys.executable, '-m', 'streamlit', 'run',
        str(app_file),
        '--server.port', '8502',
        '--server.address', '127.0.0.1'
    ]
    
    try:
        print("üåê Interface disponible sur: http://localhost:8502")
        streamlit_process = subprocess.run(cmd)
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Arr√™t de Vekta V2")

def cleanup_processes():
    """Nettoie les processus avant de quitter"""
    global ollama_process, streamlit_process
    
    print("\nüßπ Nettoyage des processus...")
    
    if streamlit_process and streamlit_process.poll() is None:
        print("‚èπÔ∏è  Arr√™t de Streamlit...")
        streamlit_process.terminate()
    
    if ollama_process and ollama_process.poll() is None:
        print("‚èπÔ∏è  Arr√™t d'Ollama...")
        ollama_process.terminate()
        try:
            ollama_process.wait(timeout=5)
        except subprocess.TimeoutExpired:
            ollama_process.kill()

def signal_handler(signum, frame):
    """Gestionnaire de signaux pour un arr√™t propre"""
    cleanup_processes()
    sys.exit(0)

def check_system_resources():
    """V√©rifie les ressources syst√®me pour Mac M3"""
    print("üñ•Ô∏è  V√©rification des ressources syst√®me...")
    
    try:
        import psutil
        
        # V√©rifier la m√©moire disponible
        memory = psutil.virtual_memory()
        available_gb = memory.available / (1024**3)
        
        print(f"   üíæ M√©moire disponible: {available_gb:.1f} GB")
        
        if available_gb < 4:
            print("   ‚ö†Ô∏è  Attention: M√©moire faible (<4GB). Le mod√®le pourrait √™tre lent.")
        elif available_gb >= 8:
            print("   ‚úÖ M√©moire suffisante pour une performance optimale")
        else:
            print("   ‚úÖ M√©moire correcte pour le mod√®le llama3.2:3b")
            
        # V√©rifier le processeur
        cpu_count = psutil.cpu_count()
        print(f"   üî¢ C≈ìurs CPU: {cpu_count}")
        
        if cpu_count >= 8:
            print("   ‚úÖ Processeur performant d√©tect√© (optimal pour M3)")
        
        return True
        
    except ImportError:
        print("   üí° Package psutil non disponible (optionnel)")
        return True
    except Exception as e:
        print(f"   ‚ö†Ô∏è  Impossible de v√©rifier les ressources: {e}")
        return True

def main():
    """Point d'entr√©e principal"""
    print("üß† VEKTA V2 - Intelligence Architecture pour Mac M3")
    print("=" * 60)
    
    # Configurer les gestionnaires de signaux
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # Changer vers le r√©pertoire du script
    script_dir = Path(__file__).parent
    os.chdir(script_dir)
    
    try:
        # 1. V√©rifier les ressources syst√®me
        check_system_resources()
        print()
        
        # 2. V√©rifier les d√©pendances Python
        check_dependencies()
        print()
        
        # 3. Configurer Ollama (installation + d√©marrage)
        if not setup_ollama():
            print("‚ùå Impossible de configurer Ollama")
            sys.exit(1)
        print()
        
        # 4. Lancer Streamlit
        launch_streamlit_v2()
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Arr√™t demand√© par l'utilisateur")
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        sys.exit(1)
    finally:
        cleanup_processes()

if __name__ == "__main__":
    main()
