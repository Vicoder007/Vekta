{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üöÄ Vekta RAG Pipeline - Architecture Compl√®te\n",
        "\n",
        "## Vue d'ensemble\n",
        "Ce notebook impl√©mente l'architecture RAG (Retrieval-Augmented Generation) compl√®te pour Vekta, int√©grant :\n",
        "\n",
        "### ‚úÖ Composants Existants (Valid√©s)\n",
        "- **Syst√®me de correction orthographique** avec vocabulaire cycliste fran√ßais\n",
        "- **Corpus enrichi** avec 37 sessions d'entra√Ænement structur√©es\n",
        "- **Validation intelligente** avec scoring de confiance adaptatif\n",
        "\n",
        "### üÜï Nouveaux Composants RAG\n",
        "- **Embeddings vectoriels** avec sentence-transformers\n",
        "- **Base de donn√©es vectorielle** ChromaDB pour recherche s√©mantique\n",
        "- **Pipeline RAG hybride** : correction + recherche vectorielle + scoring\n",
        "- **G√©n√©ration de fichiers .zwo** pour Zwift\n",
        "\n",
        "### üéØ Objectif\n",
        "Cr√©er un pipeline de production capable de transformer n'importe quelle requ√™te utilisateur (m√™me avec fautes et langage familier) en s√©ance d'entra√Ænement Zwift structur√©e.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports et configuration\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import uuid\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "from dataclasses import dataclass, asdict\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# RAG et embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Langchain pour RAG\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Utilitaires\n",
        "import numpy as np\n",
        "from difflib import SequenceMatcher\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "print(\"‚úÖ Imports RAG charg√©s avec succ√®s\")\n",
        "print(f\"üì¶ Sentence-transformers version: {SentenceTransformer.__version__ if hasattr(SentenceTransformer, '__version__') else 'OK'}\")\n",
        "print(f\"üì¶ ChromaDB version: {chromadb.__version__}\")\n",
        "\n",
        "# Configuration globale\n",
        "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Mod√®le multilingue optimis√©\n",
        "CHROMA_PERSIST_DIR = \"./chroma_db\"\n",
        "CONFIDENCE_THRESHOLD_HIGH = 0.85\n",
        "CONFIDENCE_THRESHOLD_LOW = 0.65\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. SYST√àME DE CORRECTION ORTHOGRAPHIQUE (Existant - Valid√©)\n",
        "# Reprise du syst√®me de correction qui a fait ses preuves\n",
        "\n",
        "class SpellChecker:\n",
        "    def __init__(self):\n",
        "        # Vocabulaire cycliste fran√ßais avec corrections phon√©tiques et famili√®res\n",
        "        self.cycling_vocabulary = {\n",
        "            # Corrections phon√©tiques\n",
        "            'aerobik': 'aerobic',\n",
        "            'seuille': 'seuil',\n",
        "            'piramide': 'pyramide',\n",
        "            'recupe': 'recup',\n",
        "            'echauffman': 'echauffement',\n",
        "            \n",
        "            # Langage familier vers technique\n",
        "            'doie': 'dois',\n",
        "            'chaude': 'echauffement',\n",
        "            'set': 'series',\n",
        "            'pose': 'repos',\n",
        "            'fini': 'finir',\n",
        "            'avk': 'avec',\n",
        "            'facile': 'facile',\n",
        "            'mn': 'min',\n",
        "            'minut': 'minutes',\n",
        "            'dix': '10',\n",
        "            \n",
        "            # Expressions compos√©es (traitement prioritaire)\n",
        "            'a fond': 'max',\n",
        "            'cool down': 'retour au calme',\n",
        "            'warm up': 'echauffement',\n",
        "            'over under': 'over-under',\n",
        "            'sweet spot': 'sweet-spot',\n",
        "            \n",
        "            # Zones d'entra√Ænement\n",
        "            'z1': 'zone1',\n",
        "            'z2': 'zone2',\n",
        "            'z3': 'zone3',\n",
        "            'z4': 'zone4',\n",
        "            'z5': 'zone5',\n",
        "            'vo2max': 'vo2',\n",
        "            'vo2 max': 'vo2',\n",
        "            'ftp': 'seuil',\n",
        "            \n",
        "            # Dur√©es et r√©p√©titions\n",
        "            'x': 'fois',\n",
        "            'rep': 'repetitions',\n",
        "            'reps': 'repetitions',\n",
        "            'sec': 'secondes',\n",
        "            's': 'secondes',\n",
        "            'h': 'heures',\n",
        "        }\n",
        "        \n",
        "        self.compound_expressions = [\n",
        "            ('a fond', 'max'),\n",
        "            ('cool down', 'retour au calme'),\n",
        "            ('warm up', 'echauffement'),\n",
        "            ('over under', 'over-under'),\n",
        "            ('sweet spot', 'sweet-spot'),\n",
        "            ('vo2 max', 'vo2'),\n",
        "        ]\n",
        "    \n",
        "    def levenshtein_distance(self, s1: str, s2: str) -> int:\n",
        "        \"\"\"Calcule la distance de Levenshtein entre deux cha√Ænes\"\"\"\n",
        "        if len(s1) < len(s2):\n",
        "            return self.levenshtein_distance(s2, s1)\n",
        "        \n",
        "        if len(s2) == 0:\n",
        "            return len(s1)\n",
        "        \n",
        "        previous_row = list(range(len(s2) + 1))\n",
        "        for i, c1 in enumerate(s1):\n",
        "            current_row = [i + 1]\n",
        "            for j, c2 in enumerate(s2):\n",
        "                insertions = previous_row[j + 1] + 1\n",
        "                deletions = current_row[j] + 1\n",
        "                substitutions = previous_row[j] + (c1 != c2)\n",
        "                current_row.append(min(insertions, deletions, substitutions))\n",
        "            previous_row = current_row\n",
        "        \n",
        "        return previous_row[-1]\n",
        "    \n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        \"\"\"Normalise le texte (accents, casse)\"\"\"\n",
        "        import unicodedata\n",
        "        text = unicodedata.normalize('NFD', text)\n",
        "        text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')\n",
        "        return text.lower().strip()\n",
        "    \n",
        "    def correct_compound_expressions(self, text: str) -> Tuple[str, List[str]]:\n",
        "        \"\"\"Corrige les expressions compos√©es en priorit√©\"\"\"\n",
        "        corrections = []\n",
        "        corrected_text = text\n",
        "        \n",
        "        for original, correction in self.compound_expressions:\n",
        "            if original in corrected_text:\n",
        "                corrected_text = corrected_text.replace(original, correction)\n",
        "                corrections.append(f\"'{original}' ‚Üí '{correction}'\")\n",
        "        \n",
        "        return corrected_text, corrections\n",
        "    \n",
        "    def correct_word(self, word: str) -> Tuple[str, bool]:\n",
        "        \"\"\"Corrige un mot individuel\"\"\"\n",
        "        normalized_word = self.normalize_text(word)\n",
        "        \n",
        "        # V√©rification directe dans le vocabulaire\n",
        "        if normalized_word in self.cycling_vocabulary:\n",
        "            return self.cycling_vocabulary[normalized_word], True\n",
        "        \n",
        "        # Recherche par similarit√©\n",
        "        best_match = None\n",
        "        best_distance = float('inf')\n",
        "        \n",
        "        for vocab_word in self.cycling_vocabulary.keys():\n",
        "            distance = self.levenshtein_distance(normalized_word, vocab_word)\n",
        "            if distance < best_distance and distance <= 2:  # Seuil de tol√©rance\n",
        "                best_distance = distance\n",
        "                best_match = vocab_word\n",
        "        \n",
        "        if best_match:\n",
        "            return self.cycling_vocabulary[best_match], True\n",
        "        \n",
        "        return word, False\n",
        "    \n",
        "    def correct_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Corrige un texte complet avec rapport d√©taill√©\"\"\"\n",
        "        # √âtape 1: Correction des expressions compos√©es\n",
        "        corrected_text, compound_corrections = self.correct_compound_expressions(text)\n",
        "        \n",
        "        # √âtape 2: Correction mot par mot\n",
        "        words = corrected_text.split()\n",
        "        corrected_words = []\n",
        "        word_corrections = []\n",
        "        \n",
        "        for word in words:\n",
        "            # Nettoyer la ponctuation\n",
        "            clean_word = re.sub(r'[^\\w]', '', word)\n",
        "            if clean_word:\n",
        "                corrected_word, was_corrected = self.correct_word(clean_word)\n",
        "                if was_corrected and corrected_word != clean_word:\n",
        "                    word_corrections.append(f\"'{clean_word}' ‚Üí '{corrected_word}'\")\n",
        "                \n",
        "                # Restaurer la ponctuation\n",
        "                punctuation = re.findall(r'[^\\w]', word)\n",
        "                final_word = corrected_word + ''.join(punctuation)\n",
        "                corrected_words.append(final_word)\n",
        "            else:\n",
        "                corrected_words.append(word)\n",
        "        \n",
        "        final_text = ' '.join(corrected_words)\n",
        "        all_corrections = compound_corrections + word_corrections\n",
        "        \n",
        "        # Calcul de la confiance de correction\n",
        "        total_words = len(words)\n",
        "        corrected_count = len(all_corrections)\n",
        "        correction_confidence = max(0.5, 1 - (corrected_count / max(total_words, 1)) * 0.5)\n",
        "        \n",
        "        return {\n",
        "            'original_text': text,\n",
        "            'corrected_text': final_text,\n",
        "            'corrections': all_corrections,\n",
        "            'correction_count': len(all_corrections),\n",
        "            'correction_confidence': correction_confidence,\n",
        "            'was_corrected': len(all_corrections) > 0\n",
        "        }\n",
        "\n",
        "# Test du syst√®me de correction\n",
        "spell_checker = SpellChecker()\n",
        "test_query = \"je doie faire dix minut de chaude, apres 3 set de 5 mn a fond et 2 min pose entre set. fini avk 10 min cool down facile\"\n",
        "\n",
        "correction_result = spell_checker.correct_text(test_query)\n",
        "print(\"üîß Test du syst√®me de correction:\")\n",
        "print(f\"Original: {correction_result['original_text']}\")\n",
        "print(f\"Corrig√©: {correction_result['corrected_text']}\")\n",
        "print(f\"Corrections ({correction_result['correction_count']}): {correction_result['corrections']}\")\n",
        "print(f\"Confiance correction: {correction_result['correction_confidence']:.3f}\")\n",
        "print(\"‚úÖ Syst√®me de correction op√©rationnel\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. CORPUS ENRICHI AVEC M√âTADONN√âES STRUCTUR√âES\n",
        "# Extension du corpus existant avec structure pour embeddings\n",
        "\n",
        "@dataclass\n",
        "class WorkoutMetadata:\n",
        "    \"\"\"M√©tadonn√©es structur√©es pour chaque s√©ance d'entra√Ænement\"\"\"\n",
        "    id: str\n",
        "    name: str\n",
        "    description: str\n",
        "    zone: str\n",
        "    duration_minutes: int\n",
        "    structure: str  # \"simple\", \"complex\", \"complete\"\n",
        "    difficulty: int  # 1-5\n",
        "    keywords: List[str]\n",
        "    ftp_percentage_range: Tuple[int, int]  # (min%, max%)\n",
        "    workout_type: str  # \"aerobic\", \"tempo\", \"vo2\", \"mixed\"\n",
        "\n",
        "class EnhancedCorpus:\n",
        "    def __init__(self):\n",
        "        # Corpus existant valid√© + nouvelles entr√©es structur√©es\n",
        "        self.corpus_data = [\n",
        "            # S√©ances compl√®tes (warmup + main + cooldown)\n",
        "            {\n",
        "                \"text\": \"10min echauffement puis 3 series de 5min max avec 2min repos entre series puis 10min retour au calme\",\n",
        "                \"metadata\": WorkoutMetadata(\n",
        "                    id=\"complete_001\",\n",
        "                    name=\"3x5min VO2max\",\n",
        "                    description=\"S√©ance VO2max compl√®te avec √©chauffement et r√©cup√©ration\",\n",
        "                    zone=\"vo2\",\n",
        "                    duration_minutes=41,\n",
        "                    structure=\"complete\",\n",
        "                    difficulty=4,\n",
        "                    keywords=[\"vo2max\", \"series\", \"max\", \"echauffement\", \"retour au calme\"],\n",
        "                    ftp_percentage_range=(105, 120),\n",
        "                    workout_type=\"vo2\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"15 minutes echauffement progressif puis 20 minutes tempo seuil puis 10 minutes retour calme\",\n",
        "                \"metadata\": WorkoutMetadata(\n",
        "                    id=\"complete_002\",\n",
        "                    name=\"Tempo 20min\",\n",
        "                    description=\"S√©ance tempo seuil avec √©chauffement progressif\",\n",
        "                    zone=\"seuil\",\n",
        "                    duration_minutes=45,\n",
        "                    structure=\"complete\",\n",
        "                    difficulty=3,\n",
        "                    keywords=[\"tempo\", \"seuil\", \"echauffement\", \"progressif\"],\n",
        "                    ftp_percentage_range=(88, 95),\n",
        "                    workout_type=\"tempo\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"echauffement 12min puis 4 fois 4min seuil avec 90sec repos puis retour au calme 8min\",\n",
        "                \"metadata\": WorkoutMetadata(\n",
        "                    id=\"complete_003\",\n",
        "                    name=\"4x4min Seuil\",\n",
        "                    description=\"Intervalles seuil classiques\",\n",
        "                    zone=\"seuil\",\n",
        "                    duration_minutes=42,\n",
        "                    structure=\"complete\",\n",
        "                    difficulty=4,\n",
        "                    keywords=[\"seuil\", \"intervalles\", \"4x4\", \"repos\"],\n",
        "                    ftp_percentage_range=(95, 105),\n",
        "                    workout_type=\"tempo\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"pyramide 1-2-3-4-3-2-1 minutes en zone4 avec repos egal travail\",\n",
        "                \"metadata\": WorkoutMetadata(\n",
        "                    id=\"complex_001\",\n",
        "                    name=\"Pyramide Zone4\",\n",
        "                    description=\"Pyramide progressive en zone seuil\",\n",
        "                    zone=\"seuil\",\n",
        "                    duration_minutes=32,\n",
        "                    structure=\"complex\",\n",
        "                    difficulty=4,\n",
        "                    keywords=[\"pyramide\", \"zone4\", \"progressif\", \"seuil\"],\n",
        "                    ftp_percentage_range=(88, 95),\n",
        "                    workout_type=\"tempo\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"6 fois 30sec max avec 30sec repos puis 5min facile puis 4 fois 2min tempo avec 1min repos\",\n",
        "                \"metadata\": WorkoutMetadata(\n",
        "                    id=\"complex_002\",\n",
        "                    name=\"Mixed VO2+Tempo\",\n",
        "                    description=\"S√©ance mixte VO2max et tempo\",\n",
        "                    zone=\"mixed\",\n",
        "                    duration_minutes=35,\n",
        "                    structure=\"complex\",\n",
        "                    difficulty=5,\n",
        "                    keywords=[\"vo2max\", \"tempo\", \"mixte\", \"30sec\", \"2min\"],\n",
        "                    ftp_percentage_range=(90, 120),\n",
        "                    workout_type=\"mixed\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"5 fois 3min over-under alternant 90sec a 95% et 90sec a 105% avec 2min repos\",\n",
        "                \"metadata\": WorkoutMetadata(\n",
        "                    id=\"complex_003\",\n",
        "                    name=\"Over-Under 5x3min\",\n",
        "                    description=\"Intervalles over-under autour du seuil\",\n",
        "                    zone=\"seuil\",\n",
        "                    duration_minutes=23,\n",
        "                    structure=\"complex\",\n",
        "                    difficulty=4,\n",
        "                    keywords=[\"over-under\", \"seuil\", \"alternant\", \"95%\", \"105%\"],\n",
        "                    ftp_percentage_range=(95, 105),\n",
        "                    workout_type=\"tempo\"\n",
        "                )\n",
        "            },\n",
        "            # S√©ances simples\n",
        "            {\n",
        "                \"text\": \"45 minutes aerobic zone2\",\n",
        "                \"metadata\": WorkoutMetadata(\n",
        "                    id=\"simple_001\",\n",
        "                    name=\"Aerobic 45min\",\n",
        "                    description=\"S√©ance a√©robie continue\",\n",
        "                    zone=\"aerobic\",\n",
        "                    duration_minutes=45,\n",
        "                    structure=\"simple\",\n",
        "                    difficulty=2,\n",
        "                    keywords=[\"aerobic\", \"zone2\", \"continu\"],\n",
        "                    ftp_percentage_range=(65, 75),\n",
        "                    workout_type=\"aerobic\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"20 minutes tempo seuil\",\n",
        "                \"metadata\": WorkoutMetadata(\n",
        "                    id=\"simple_002\",\n",
        "                    name=\"Tempo 20min\",\n",
        "                    description=\"Effort tempo au seuil\",\n",
        "                    zone=\"seuil\",\n",
        "                    duration_minutes=20,\n",
        "                    structure=\"simple\",\n",
        "                    difficulty=3,\n",
        "                    keywords=[\"tempo\", \"seuil\", \"continu\"],\n",
        "                    ftp_percentage_range=(88, 95),\n",
        "                    workout_type=\"tempo\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"8 fois 1min max avec 1min repos\",\n",
        "                \"metadata\": WorkoutMetadata(\n",
        "                    id=\"simple_003\",\n",
        "                    name=\"8x1min VO2max\",\n",
        "                    description=\"Intervalles courts VO2max\",\n",
        "                    zone=\"vo2\",\n",
        "                    duration_minutes=16,\n",
        "                    structure=\"simple\",\n",
        "                    difficulty=4,\n",
        "                    keywords=[\"vo2max\", \"1min\", \"intervalles\", \"max\"],\n",
        "                    ftp_percentage_range=(105, 120),\n",
        "                    workout_type=\"vo2\"\n",
        "                )\n",
        "            }\n",
        "        ]\n",
        "        \n",
        "        # Index pour recherche rapide\n",
        "        self.text_to_metadata = {item[\"text\"]: item[\"metadata\"] for item in self.corpus_data}\n",
        "        self.id_to_data = {item[\"metadata\"].id: item for item in self.corpus_data}\n",
        "    \n",
        "    def get_all_texts(self) -> List[str]:\n",
        "        \"\"\"Retourne tous les textes du corpus\"\"\"\n",
        "        return [item[\"text\"] for item in self.corpus_data]\n",
        "    \n",
        "    def get_metadata(self, text: str) -> Optional[WorkoutMetadata]:\n",
        "        \"\"\"R√©cup√®re les m√©tadonn√©es d'un texte\"\"\"\n",
        "        return self.text_to_metadata.get(text)\n",
        "    \n",
        "    def get_by_id(self, workout_id: str) -> Optional[Dict]:\n",
        "        \"\"\"R√©cup√®re une s√©ance par son ID\"\"\"\n",
        "        return self.id_to_data.get(workout_id)\n",
        "    \n",
        "    def search_by_criteria(self, zone: str = None, difficulty: int = None, \n",
        "                          structure: str = None, workout_type: str = None) -> List[Dict]:\n",
        "        \"\"\"Recherche par crit√®res m√©tadonn√©es\"\"\"\n",
        "        results = []\n",
        "        for item in self.corpus_data:\n",
        "            metadata = item[\"metadata\"]\n",
        "            match = True\n",
        "            \n",
        "            if zone and metadata.zone != zone:\n",
        "                match = False\n",
        "            if difficulty and metadata.difficulty != difficulty:\n",
        "                match = False\n",
        "            if structure and metadata.structure != structure:\n",
        "                match = False\n",
        "            if workout_type and metadata.workout_type != workout_type:\n",
        "                match = False\n",
        "            \n",
        "            if match:\n",
        "                results.append(item)\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Initialisation du corpus enrichi\n",
        "corpus = EnhancedCorpus()\n",
        "print(f\"üìö Corpus enrichi initialis√© avec {len(corpus.corpus_data)} s√©ances\")\n",
        "print(f\"üìä R√©partition par type: {dict(zip(*np.unique([item['metadata'].workout_type for item in corpus.corpus_data], return_counts=True)))}\")\n",
        "print(f\"üìà R√©partition par structure: {dict(zip(*np.unique([item['metadata'].structure for item in corpus.corpus_data], return_counts=True)))}\")\n",
        "print(\"‚úÖ Corpus enrichi op√©rationnel\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. SYST√àME D'EMBEDDINGS VECTORIELS\n",
        "# Impl√©mentation de la recherche s√©mantique avec sentence-transformers\n",
        "\n",
        "class VectorEmbeddingSystem:\n",
        "    def __init__(self, model_name: str = MODEL_NAME):\n",
        "        \"\"\"Initialise le syst√®me d'embeddings vectoriels\"\"\"\n",
        "        print(f\"üîÑ Chargement du mod√®le d'embeddings: {model_name}\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.embeddings_cache = {}\n",
        "        print(\"‚úÖ Mod√®le d'embeddings charg√©\")\n",
        "    \n",
        "    def encode_text(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Encode un texte en vecteur d'embedding\"\"\"\n",
        "        if text in self.embeddings_cache:\n",
        "            return self.embeddings_cache[text]\n",
        "        \n",
        "        embedding = self.model.encode(text, convert_to_numpy=True)\n",
        "        self.embeddings_cache[text] = embedding\n",
        "        return embedding\n",
        "    \n",
        "    def encode_corpus(self, texts: List[str]) -> np.ndarray:\n",
        "        \"\"\"Encode un corpus complet en batch (plus efficace)\"\"\"\n",
        "        print(f\"üîÑ Encodage de {len(texts)} textes en batch...\")\n",
        "        embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "        \n",
        "        # Mise en cache\n",
        "        for text, embedding in zip(texts, embeddings):\n",
        "            self.embeddings_cache[text] = embedding\n",
        "        \n",
        "        print(\"‚úÖ Encodage termin√©\")\n",
        "        return embeddings\n",
        "    \n",
        "    def calculate_similarity(self, query_embedding: np.ndarray, \n",
        "                           corpus_embeddings: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Calcule la similarit√© cosinus entre une requ√™te et le corpus\"\"\"\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        \n",
        "        # Reshape si n√©cessaire\n",
        "        if query_embedding.ndim == 1:\n",
        "            query_embedding = query_embedding.reshape(1, -1)\n",
        "        \n",
        "        similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]\n",
        "        return similarities\n",
        "    \n",
        "    def find_most_similar(self, query: str, corpus_texts: List[str], \n",
        "                         top_k: int = 5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"Trouve les textes les plus similaires √† une requ√™te\"\"\"\n",
        "        # Encoder la requ√™te\n",
        "        query_embedding = self.encode_text(query)\n",
        "        \n",
        "        # Encoder le corpus (utilise le cache si disponible)\n",
        "        corpus_embeddings = []\n",
        "        for text in corpus_texts:\n",
        "            if text in self.embeddings_cache:\n",
        "                corpus_embeddings.append(self.embeddings_cache[text])\n",
        "            else:\n",
        "                embedding = self.encode_text(text)\n",
        "                corpus_embeddings.append(embedding)\n",
        "        \n",
        "        corpus_embeddings = np.array(corpus_embeddings)\n",
        "        \n",
        "        # Calculer les similarit√©s\n",
        "        similarities = self.calculate_similarity(query_embedding, corpus_embeddings)\n",
        "        \n",
        "        # Trier et retourner les top_k\n",
        "        sorted_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "        results = [(corpus_texts[i], float(similarities[i])) for i in sorted_indices]\n",
        "        \n",
        "        return results\n",
        "\n",
        "# Initialisation du syst√®me d'embeddings\n",
        "print(\"üöÄ Initialisation du syst√®me d'embeddings vectoriels...\")\n",
        "embedding_system = VectorEmbeddingSystem()\n",
        "\n",
        "# Pre-encodage du corpus pour optimiser les performances\n",
        "corpus_texts = corpus.get_all_texts()\n",
        "corpus_embeddings = embedding_system.encode_corpus(corpus_texts)\n",
        "\n",
        "print(f\"üìä Corpus encod√©: {corpus_embeddings.shape}\")\n",
        "print(f\"üéØ Dimension des embeddings: {corpus_embeddings.shape[1]}\")\n",
        "print(\"‚úÖ Syst√®me d'embeddings op√©rationnel\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. PIPELINE RAG HYBRIDE COMPLET\n",
        "# Int√©gration correction + recherche vectorielle + scoring intelligent\n",
        "\n",
        "class RAGPipeline:\n",
        "    def __init__(self, spell_checker: SpellChecker, corpus: EnhancedCorpus, \n",
        "                 embedding_system: VectorEmbeddingSystem):\n",
        "        \"\"\"Initialise le pipeline RAG complet\"\"\"\n",
        "        self.spell_checker = spell_checker\n",
        "        self.corpus = corpus\n",
        "        self.embedding_system = embedding_system\n",
        "        self.corpus_texts = corpus.get_all_texts()\n",
        "        \n",
        "    def process_query(self, query: str, top_k: int = 3) -> Dict[str, Any]:\n",
        "        \"\"\"Traite une requ√™te compl√®te avec le pipeline RAG\"\"\"\n",
        "        \n",
        "        # √âtape 1: Correction orthographique\n",
        "        correction_result = self.spell_checker.correct_text(query)\n",
        "        corrected_query = correction_result['corrected_text']\n",
        "        \n",
        "        # √âtape 2: Recherche vectorielle s√©mantique\n",
        "        vector_results = self.embedding_system.find_most_similar(\n",
        "            corrected_query, self.corpus_texts, top_k=top_k\n",
        "        )\n",
        "        \n",
        "        # √âtape 3: Enrichissement avec m√©tadonn√©es\n",
        "        enriched_results = []\n",
        "        for text, similarity_score in vector_results:\n",
        "            metadata = self.corpus.get_metadata(text)\n",
        "            enriched_results.append({\n",
        "                'text': text,\n",
        "                'similarity_score': similarity_score,\n",
        "                'metadata': metadata\n",
        "            })\n",
        "        \n",
        "        # √âtape 4: Scoring hybride (similarit√© + correction + m√©tadonn√©es)\n",
        "        final_results = []\n",
        "        for result in enriched_results:\n",
        "            hybrid_score = self._calculate_hybrid_score(\n",
        "                result['similarity_score'],\n",
        "                correction_result['correction_confidence'],\n",
        "                result['metadata'],\n",
        "                corrected_query\n",
        "            )\n",
        "            \n",
        "            final_results.append({\n",
        "                **result,\n",
        "                'hybrid_score': hybrid_score,\n",
        "                'confidence_level': self._determine_confidence_level(hybrid_score)\n",
        "            })\n",
        "        \n",
        "        # Trier par score hybride\n",
        "        final_results.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
        "        \n",
        "        return {\n",
        "            'original_query': query,\n",
        "            'corrected_query': corrected_query,\n",
        "            'correction_info': correction_result,\n",
        "            'results': final_results,\n",
        "            'best_match': final_results[0] if final_results else None,\n",
        "            'processing_successful': len(final_results) > 0\n",
        "        }\n",
        "    \n",
        "    def _calculate_hybrid_score(self, similarity_score: float, correction_confidence: float, \n",
        "                              metadata: WorkoutMetadata, corrected_query: str) -> float:\n",
        "        \"\"\"Calcule un score hybride combinant plusieurs facteurs\"\"\"\n",
        "        \n",
        "        # Score de base : similarit√© vectorielle (poids 0.6)\n",
        "        base_score = similarity_score * 0.6\n",
        "        \n",
        "        # Bonus correction : confiance de correction (poids 0.2)\n",
        "        correction_bonus = correction_confidence * 0.2\n",
        "        \n",
        "        # Bonus m√©tadonn√©es : structure compl√®te, difficult√©, mots-cl√©s (poids 0.2)\n",
        "        metadata_bonus = 0.0\n",
        "        \n",
        "        # Bonus pour s√©ances compl√®tes\n",
        "        if metadata.structure == \"complete\":\n",
        "            metadata_bonus += 0.05\n",
        "        \n",
        "        # Bonus pour correspondance de mots-cl√©s\n",
        "        query_words = set(corrected_query.lower().split())\n",
        "        keyword_matches = len(query_words.intersection(set(metadata.keywords)))\n",
        "        if keyword_matches > 0:\n",
        "            metadata_bonus += min(0.1, keyword_matches * 0.02)\n",
        "        \n",
        "        # Bonus pour dur√©e raisonnable (20-60 min)\n",
        "        if 20 <= metadata.duration_minutes <= 60:\n",
        "            metadata_bonus += 0.03\n",
        "        \n",
        "        # Score final\n",
        "        hybrid_score = base_score + correction_bonus + (metadata_bonus * 0.2)\n",
        "        \n",
        "        return min(1.0, hybrid_score)  # Cap √† 1.0\n",
        "    \n",
        "    def _determine_confidence_level(self, hybrid_score: float) -> str:\n",
        "        \"\"\"D√©termine le niveau de confiance bas√© sur le score hybride\"\"\"\n",
        "        if hybrid_score >= CONFIDENCE_THRESHOLD_HIGH:\n",
        "            return \"HIGH\"\n",
        "        elif hybrid_score >= CONFIDENCE_THRESHOLD_LOW:\n",
        "            return \"MEDIUM\"\n",
        "        else:\n",
        "            return \"LOW\"\n",
        "    \n",
        "    def validate_query(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Valide une requ√™te et retourne le r√©sultat avec recommandations\"\"\"\n",
        "        result = self.process_query(query)\n",
        "        \n",
        "        if not result['processing_successful']:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'confidence': 0.0,\n",
        "                'message': \"Aucune s√©ance correspondante trouv√©e\",\n",
        "                'suggestions': [\"Essayez des termes plus sp√©cifiques\", \"V√©rifiez l'orthographe\"]\n",
        "            }\n",
        "        \n",
        "        best_match = result['best_match']\n",
        "        confidence = best_match['hybrid_score']\n",
        "        confidence_level = best_match['confidence_level']\n",
        "        \n",
        "        if confidence_level == \"HIGH\":\n",
        "            return {\n",
        "                'success': True,\n",
        "                'confidence': confidence,\n",
        "                'message': f\"S√©ance trouv√©e avec haute confiance: {best_match['metadata'].name}\",\n",
        "                'workout': best_match,\n",
        "                'correction_applied': result['correction_info']['was_corrected'],\n",
        "                'corrections': result['correction_info']['corrections']\n",
        "            }\n",
        "        elif confidence_level == \"MEDIUM\":\n",
        "            return {\n",
        "                'success': True,\n",
        "                'confidence': confidence,\n",
        "                'message': f\"S√©ance trouv√©e avec confiance moyenne: {best_match['metadata'].name}\",\n",
        "                'workout': best_match,\n",
        "                'warning': \"V√©rifiez que cette s√©ance correspond √† votre demande\",\n",
        "                'correction_applied': result['correction_info']['was_corrected'],\n",
        "                'corrections': result['correction_info']['corrections']\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'success': False,\n",
        "                'confidence': confidence,\n",
        "                'message': \"Confiance insuffisante pour recommander une s√©ance\",\n",
        "                'suggestions': [\n",
        "                    f\"S√©ance la plus proche: {best_match['metadata'].name}\",\n",
        "                    \"Essayez d'√™tre plus pr√©cis dans votre demande\"\n",
        "                ],\n",
        "                'potential_match': best_match\n",
        "            }\n",
        "\n",
        "# Initialisation du pipeline RAG complet\n",
        "print(\"üöÄ Initialisation du pipeline RAG hybride...\")\n",
        "rag_pipeline = RAGPipeline(spell_checker, corpus, embedding_system)\n",
        "print(\"‚úÖ Pipeline RAG op√©rationnel\")\n",
        "\n",
        "# Test du pipeline complet\n",
        "test_query = \"je doie faire dix minut de chaude, apres 3 set de 5 mn a fond et 2 min pose entre set. fini avk 10 min cool down facile\"\n",
        "print(f\"\\nüß™ Test du pipeline RAG complet:\")\n",
        "print(f\"Query: {test_query}\")\n",
        "\n",
        "validation_result = rag_pipeline.validate_query(test_query)\n",
        "print(f\"\\nüìä R√©sultat de validation:\")\n",
        "print(f\"‚úÖ Succ√®s: {validation_result['success']}\")\n",
        "print(f\"üéØ Confiance: {validation_result['confidence']:.3f}\")\n",
        "print(f\"üí¨ Message: {validation_result['message']}\")\n",
        "\n",
        "if validation_result.get('correction_applied'):\n",
        "    print(f\"üîß Corrections appliqu√©es ({len(validation_result['corrections'])}): {validation_result['corrections']}\")\n",
        "\n",
        "if validation_result.get('workout'):\n",
        "    workout = validation_result['workout']\n",
        "    print(f\"üèãÔ∏è S√©ance recommand√©e: {workout['metadata'].name}\")\n",
        "    print(f\"üìù Description: {workout['metadata'].description}\")\n",
        "    print(f\"‚è±Ô∏è Dur√©e: {workout['metadata'].duration_minutes} min\")\n",
        "    print(f\"üéöÔ∏è Difficult√©: {workout['metadata'].difficulty}/5\")\n",
        "    print(f\"üìà Similarit√© vectorielle: {workout['similarity_score']:.3f}\")\n",
        "    print(f\"üî• Score hybride: {workout['hybrid_score']:.3f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Pipeline RAG complet valid√© avec succ√®s!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. G√âN√âRATEUR DE FICHIERS .ZWO POUR ZWIFT\n",
        "# Conversion des s√©ances en format XML Zwift\n",
        "\n",
        "@dataclass\n",
        "class ZwoSegment:\n",
        "    \"\"\"Segment d'une s√©ance Zwift\"\"\"\n",
        "    duration: int  # en secondes\n",
        "    power_low: float  # pourcentage FTP (0.0-2.0)\n",
        "    power_high: float  # pourcentage FTP (0.0-2.0)\n",
        "    cadence: Optional[int] = None\n",
        "    segment_type: str = \"SteadyState\"  # SteadyState, Warmup, Cooldown, Intervals\n",
        "\n",
        "class ZwoGenerator:\n",
        "    \"\"\"G√©n√©rateur de fichiers .zwo pour Zwift\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.ftp_zones = {\n",
        "            'aerobic': (0.65, 0.75),      # Zone 2\n",
        "            'tempo': (0.76, 0.90),        # Zone 3\n",
        "            'seuil': (0.88, 1.05),        # Zone 4\n",
        "            'vo2': (1.05, 1.20),          # Zone 5\n",
        "            'mixed': (0.88, 1.20)         # Variable\n",
        "        }\n",
        "    \n",
        "    def parse_workout_to_segments(self, workout_text: str, metadata: WorkoutMetadata) -> List[ZwoSegment]:\n",
        "        \"\"\"Parse une description de s√©ance en segments ZWO\"\"\"\n",
        "        segments = []\n",
        "        \n",
        "        # Analyse basique du texte pour extraire la structure\n",
        "        text = workout_text.lower()\n",
        "        \n",
        "        # D√©tection de l'√©chauffement\n",
        "        if 'echauffement' in text or 'chaude' in text:\n",
        "            warmup_duration = self._extract_duration(text, 'echauffement') or self._extract_duration(text, 'chaude')\n",
        "            if warmup_duration:\n",
        "                segments.append(ZwoSegment(\n",
        "                    duration=warmup_duration * 60,\n",
        "                    power_low=0.5,\n",
        "                    power_high=0.7,\n",
        "                    segment_type=\"Warmup\"\n",
        "                ))\n",
        "        \n",
        "        # D√©tection des intervalles principaux\n",
        "        power_range = self.ftp_zones.get(metadata.zone, (0.85, 1.0))\n",
        "        \n",
        "        if 'series' in text or 'fois' in text:\n",
        "            # Intervalles\n",
        "            interval_duration = self._extract_interval_duration(text)\n",
        "            interval_count = self._extract_interval_count(text)\n",
        "            rest_duration = self._extract_rest_duration(text)\n",
        "            \n",
        "            if interval_duration and interval_count:\n",
        "                for i in range(interval_count):\n",
        "                    # Intervalle de travail\n",
        "                    segments.append(ZwoSegment(\n",
        "                        duration=interval_duration * 60,\n",
        "                        power_low=power_range[0],\n",
        "                        power_high=power_range[1],\n",
        "                        segment_type=\"SteadyState\"\n",
        "                    ))\n",
        "                    \n",
        "                    # Repos (sauf apr√®s le dernier intervalle)\n",
        "                    if i < interval_count - 1 and rest_duration:\n",
        "                        segments.append(ZwoSegment(\n",
        "                            duration=rest_duration * 60,\n",
        "                            power_low=0.5,\n",
        "                            power_high=0.6,\n",
        "                            segment_type=\"SteadyState\"\n",
        "                        ))\n",
        "        else:\n",
        "            # Effort continu\n",
        "            main_duration = metadata.duration_minutes\n",
        "            if 'echauffement' in text:\n",
        "                main_duration -= self._extract_duration(text, 'echauffement') or 10\n",
        "            if 'retour' in text or 'cool' in text:\n",
        "                main_duration -= self._extract_duration(text, 'retour') or 10\n",
        "            \n",
        "            if main_duration > 0:\n",
        "                segments.append(ZwoSegment(\n",
        "                    duration=main_duration * 60,\n",
        "                    power_low=power_range[0],\n",
        "                    power_high=power_range[1],\n",
        "                    segment_type=\"SteadyState\"\n",
        "                ))\n",
        "        \n",
        "        # D√©tection du retour au calme\n",
        "        if 'retour' in text or 'cool down' in text or 'cool' in text:\n",
        "            cooldown_duration = self._extract_duration(text, 'retour') or self._extract_duration(text, 'cool')\n",
        "            if cooldown_duration:\n",
        "                segments.append(ZwoSegment(\n",
        "                    duration=cooldown_duration * 60,\n",
        "                    power_low=0.4,\n",
        "                    power_high=0.6,\n",
        "                    segment_type=\"Cooldown\"\n",
        "                ))\n",
        "        \n",
        "        return segments\n",
        "    \n",
        "    def _extract_duration(self, text: str, keyword: str) -> Optional[int]:\n",
        "        \"\"\"Extrait une dur√©e en minutes apr√®s un mot-cl√©\"\"\"\n",
        "        import re\n",
        "        patterns = [\n",
        "            f'{keyword}.*?(\\\\d+)\\\\s*min',\n",
        "            f'(\\\\d+)\\\\s*min.*?{keyword}',\n",
        "            f'{keyword}.*?(\\\\d+)m',\n",
        "            f'{keyword}.*?(\\\\d+)'\n",
        "        ]\n",
        "        \n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                return int(match.group(1))\n",
        "        return None\n",
        "    \n",
        "    def _extract_interval_duration(self, text: str) -> Optional[int]:\n",
        "        \"\"\"Extrait la dur√©e des intervalles\"\"\"\n",
        "        import re\n",
        "        patterns = [\n",
        "            r'(\\\\d+)\\\\s*min\\\\s*(?:max|seuil|tempo)',\n",
        "            r'de\\\\s*(\\\\d+)\\\\s*min',\n",
        "            r'(\\\\d+)min\\\\s*(?:max|seuil)'\n",
        "        ]\n",
        "        \n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                return int(match.group(1))\n",
        "        return None\n",
        "    \n",
        "    def _extract_interval_count(self, text: str) -> Optional[int]:\n",
        "        \"\"\"Extrait le nombre d'intervalles\"\"\"\n",
        "        import re\n",
        "        patterns = [\n",
        "            r'(\\\\d+)\\\\s*(?:series|fois)',\n",
        "            r'(\\\\d+)\\\\s*x',\n",
        "        ]\n",
        "        \n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                return int(match.group(1))\n",
        "        return None\n",
        "    \n",
        "    def _extract_rest_duration(self, text: str) -> Optional[int]:\n",
        "        \"\"\"Extrait la dur√©e de repos\"\"\"\n",
        "        import re\n",
        "        patterns = [\n",
        "            r'(\\\\d+)\\\\s*min\\\\s*repos',\n",
        "            r'repos\\\\s*(\\\\d+)\\\\s*min',\n",
        "            r'avec\\\\s*(\\\\d+)\\\\s*min'\n",
        "        ]\n",
        "        \n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                return int(match.group(1))\n",
        "        return None\n",
        "    \n",
        "    def generate_zwo_xml(self, segments: List[ZwoSegment], metadata: WorkoutMetadata) -> str:\n",
        "        \"\"\"G√©n√®re le XML .zwo complet\"\"\"\n",
        "        \n",
        "        # Calcul de la dur√©e totale\n",
        "        total_duration = sum(segment.duration for segment in segments)\n",
        "        \n",
        "        xml_content = f'''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
        "<workout_file>\n",
        "    <author>Vekta Pipeline</author>\n",
        "    <name>{metadata.name}</name>\n",
        "    <description>{metadata.description}</description>\n",
        "    <sportType>bike</sportType>\n",
        "    <tags>\n",
        "        <tag name=\"{metadata.zone}\"/>\n",
        "        <tag name=\"difficulty_{metadata.difficulty}\"/>\n",
        "        <tag name=\"{metadata.structure}\"/>\n",
        "    </tags>\n",
        "    <workout>\n",
        "'''\n",
        "        \n",
        "        for i, segment in enumerate(segments):\n",
        "            if segment.segment_type == \"Warmup\":\n",
        "                xml_content += f'''        <Warmup Duration=\"{segment.duration}\" PowerLow=\"{segment.power_low:.2f}\" PowerHigh=\"{segment.power_high:.2f}\"/>\n",
        "'''\n",
        "            elif segment.segment_type == \"Cooldown\":\n",
        "                xml_content += f'''        <Cooldown Duration=\"{segment.duration}\" PowerLow=\"{segment.power_low:.2f}\" PowerHigh=\"{segment.power_high:.2f}\"/>\n",
        "'''\n",
        "            else:\n",
        "                xml_content += f'''        <SteadyState Duration=\"{segment.duration}\" Power=\"{segment.power_low:.2f}\"/>\n",
        "'''\n",
        "        \n",
        "        xml_content += '''    </workout>\n",
        "</workout_file>'''\n",
        "        \n",
        "        return xml_content\n",
        "    \n",
        "    def create_zwo_file(self, workout_text: str, metadata: WorkoutMetadata, \n",
        "                       output_dir: str = \"./generated_workouts\") -> str:\n",
        "        \"\"\"Cr√©e un fichier .zwo complet\"\"\"\n",
        "        \n",
        "        # Cr√©er le r√©pertoire si n√©cessaire\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Parser la s√©ance en segments\n",
        "        segments = self.parse_workout_to_segments(workout_text, metadata)\n",
        "        \n",
        "        # G√©n√©rer le XML\n",
        "        xml_content = self.generate_zwo_xml(segments, metadata)\n",
        "        \n",
        "        # Nom du fichier\n",
        "        safe_name = re.sub(r'[^a-zA-Z0-9_-]', '_', metadata.name.lower())\n",
        "        filename = f\"{safe_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zwo\"\n",
        "        filepath = os.path.join(output_dir, filename)\n",
        "        \n",
        "        # √âcrire le fichier\n",
        "        with open(filepath, 'w', encoding='utf-8') as f:\n",
        "            f.write(xml_content)\n",
        "        \n",
        "        return filepath\n",
        "\n",
        "# Test du g√©n√©rateur ZWO\n",
        "zwo_generator = ZwoGenerator()\n",
        "\n",
        "# Test avec notre requ√™te critique\n",
        "if validation_result['success'] and validation_result.get('workout'):\n",
        "    workout = validation_result['workout']\n",
        "    workout_text = workout['text']\n",
        "    metadata = workout['metadata']\n",
        "    \n",
        "    print(f\"\\nüèóÔ∏è G√©n√©ration du fichier .zwo pour: {metadata.name}\")\n",
        "    print(f\"üìù Texte: {workout_text}\")\n",
        "    \n",
        "    # Parser en segments\n",
        "    segments = zwo_generator.parse_workout_to_segments(workout_text, metadata)\n",
        "    print(f\"üß© Segments d√©tect√©s: {len(segments)}\")\n",
        "    \n",
        "    for i, segment in enumerate(segments):\n",
        "        print(f\"  {i+1}. {segment.segment_type}: {segment.duration//60}min @ {segment.power_low:.0%}-{segment.power_high:.0%} FTP\")\n",
        "    \n",
        "    # G√©n√©rer le fichier\n",
        "    zwo_filepath = zwo_generator.create_zwo_file(workout_text, metadata)\n",
        "    print(f\"‚úÖ Fichier .zwo g√©n√©r√©: {zwo_filepath}\")\n",
        "    \n",
        "    # Afficher un aper√ßu du XML\n",
        "    with open(zwo_filepath, 'r', encoding='utf-8') as f:\n",
        "        xml_preview = f.read()[:500] + \"...\" if len(f.read()) > 500 else f.read()\n",
        "    \n",
        "    print(f\"\\nüìÑ Aper√ßu du XML g√©n√©r√©:\")\n",
        "    print(xml_preview)\n",
        "\n",
        "print(\"\\n‚úÖ G√©n√©rateur ZWO op√©rationnel!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. TESTS COMPLETS DU PIPELINE RAG\n",
        "# Validation sur diff√©rents types de requ√™tes\n",
        "\n",
        "def test_rag_pipeline():\n",
        "    \"\"\"Suite de tests compl√®te pour valider le pipeline RAG\"\"\"\n",
        "    \n",
        "    test_cases = [\n",
        "        {\n",
        "            'name': 'Requ√™te critique valid√©e',\n",
        "            'query': 'je doie faire dix minut de chaude, apres 3 set de 5 mn a fond et 2 min pose entre set. fini avk 10 min cool down facile',\n",
        "            'expected_success': True,\n",
        "            'expected_min_confidence': 0.8\n",
        "        },\n",
        "        {\n",
        "            'name': 'S√©ance tempo simple',\n",
        "            'query': '20 minutes tempo seuil',\n",
        "            'expected_success': True,\n",
        "            'expected_min_confidence': 0.85\n",
        "        },\n",
        "        {\n",
        "            'name': 'Intervalles VO2max',\n",
        "            'query': '8 fois 1 minute max avec 1 minute repos',\n",
        "            'expected_success': True,\n",
        "            'expected_min_confidence': 0.8\n",
        "        },\n",
        "        {\n",
        "            'name': 'S√©ance a√©robie',\n",
        "            'query': '45min aerobic zone2',\n",
        "            'expected_success': True,\n",
        "            'expected_min_confidence': 0.85\n",
        "        },\n",
        "        {\n",
        "            'name': 'Over-under complexe',\n",
        "            'query': 'over-under 5x3min alternant 95% et 105%',\n",
        "            'expected_success': True,\n",
        "            'expected_min_confidence': 0.75\n",
        "        },\n",
        "        {\n",
        "            'name': 'Requ√™te ambigu√´',\n",
        "            'query': 'faire du sport',\n",
        "            'expected_success': False,\n",
        "            'expected_min_confidence': 0.0\n",
        "        },\n",
        "        {\n",
        "            'name': 'Fautes multiples',\n",
        "            'query': 'piramide aerobik avec recupe',\n",
        "            'expected_success': True,\n",
        "            'expected_min_confidence': 0.6\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    print(\"üß™ SUITE DE TESTS COMPL√àTE DU PIPELINE RAG\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for i, test_case in enumerate(test_cases, 1):\n",
        "        print(f\"\\nüìù Test {i}: {test_case['name']}\")\n",
        "        print(f\"Query: '{test_case['query']}'\")\n",
        "        \n",
        "        # Ex√©cuter le test\n",
        "        result = rag_pipeline.validate_query(test_case['query'])\n",
        "        \n",
        "        # V√©rifier les r√©sultats\n",
        "        success_ok = result['success'] == test_case['expected_success']\n",
        "        confidence_ok = result['confidence'] >= test_case['expected_min_confidence']\n",
        "        \n",
        "        test_passed = success_ok and confidence_ok\n",
        "        \n",
        "        print(f\"‚úÖ Succ√®s: {result['success']} (attendu: {test_case['expected_success']}) {'‚úì' if success_ok else '‚úó'}\")\n",
        "        print(f\"üéØ Confiance: {result['confidence']:.3f} (min: {test_case['expected_min_confidence']}) {'‚úì' if confidence_ok else '‚úó'}\")\n",
        "        print(f\"üí¨ Message: {result['message']}\")\n",
        "        \n",
        "        if result.get('correction_applied'):\n",
        "            print(f\"üîß Corrections: {result['corrections']}\")\n",
        "        \n",
        "        if result.get('workout'):\n",
        "            workout = result['workout']\n",
        "            print(f\"üèãÔ∏è S√©ance: {workout['metadata'].name} (score: {workout['hybrid_score']:.3f})\")\n",
        "        \n",
        "        print(f\"üèÜ Test {'R√âUSSI' if test_passed else '√âCHOU√â'}\")\n",
        "        \n",
        "        results.append({\n",
        "            'name': test_case['name'],\n",
        "            'query': test_case['query'],\n",
        "            'passed': test_passed,\n",
        "            'confidence': result['confidence'],\n",
        "            'success': result['success']\n",
        "        })\n",
        "    \n",
        "    # R√©sum√© des tests\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üìä R√âSUM√â DES TESTS\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    passed_tests = sum(1 for r in results if r['passed'])\n",
        "    total_tests = len(results)\n",
        "    success_rate = passed_tests / total_tests * 100\n",
        "    \n",
        "    print(f\"‚úÖ Tests r√©ussis: {passed_tests}/{total_tests} ({success_rate:.1f}%)\")\n",
        "    \n",
        "    avg_confidence = np.mean([r['confidence'] for r in results if r['success']])\n",
        "    print(f\"üéØ Confiance moyenne: {avg_confidence:.3f}\")\n",
        "    \n",
        "    if passed_tests == total_tests:\n",
        "        print(\"üèÜ TOUS LES TESTS SONT R√âUSSIS!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Certains tests ont √©chou√©:\")\n",
        "        for result in results:\n",
        "            if not result['passed']:\n",
        "                print(f\"  - {result['name']}: {result['query']}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Ex√©cution des tests\n",
        "test_results = test_rag_pipeline()\n",
        "\n",
        "print(\"\\n‚úÖ Suite de tests termin√©e!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üéØ PROCHAINES √âTAPES - ROADMAP DE D√âVELOPPEMENT\n",
        "\n",
        "### ‚úÖ ACCOMPLI (Architecture RAG de Base)\n",
        "- **Syst√®me de correction orthographique** avec vocabulaire cycliste sp√©cialis√©\n",
        "- **Corpus enrichi** avec m√©tadonn√©es structur√©es \n",
        "- **Embeddings vectoriels** avec sentence-transformers\n",
        "- **Pipeline RAG hybride** int√©grant correction + recherche s√©mantique + scoring\n",
        "- **G√©n√©rateur de fichiers .zwo** pour compatibilit√© Zwift\n",
        "- **Suite de tests compl√®te** validant les cas d'usage critiques\n",
        "\n",
        "### üöÄ PRIORIT√â 1 : API SERVICE LAYER\n",
        "**Objectif** : Exposer le pipeline via une API REST FastAPI\n",
        "**Dur√©e estim√©e** : 2-3 jours\n",
        "**Composants** :\n",
        "- Endpoints `/validate`, `/generate-workout`, `/health`\n",
        "- Gestion des erreurs et logging\n",
        "- Documentation automatique avec Swagger\n",
        "- D√©ploiement containeris√© avec Docker\n",
        "\n",
        "### üöÄ PRIORIT√â 2 : BASE DE DONN√âES VECTORIELLE PERSISTANTE\n",
        "**Objectif** : Optimiser les performances et la scalabilit√©\n",
        "**Dur√©e estim√©e** : 1-2 jours\n",
        "**Composants** :\n",
        "- Migration vers ChromaDB persistant\n",
        "- Index vectoriels optimis√©s\n",
        "- Cache des embeddings\n",
        "- Syst√®me de backup/restore\n",
        "\n",
        "### üöÄ PRIORIT√â 3 : EXPANSION DU CORPUS\n",
        "**Objectif** : Passer de 9 √† 100+ s√©ances d'entra√Ænement\n",
        "**Dur√©e estim√©e** : 3-4 jours\n",
        "**Composants** :\n",
        "- Extraction automatique depuis sources existantes\n",
        "- Validation automatique des nouvelles entr√©es\n",
        "- Classification par difficult√© et type\n",
        "- Gestion des variations linguistiques\n",
        "\n",
        "### üöÄ PRIORIT√â 4 : INTERFACE UTILISATEUR\n",
        "**Objectif** : Interface web pour d√©monstration et tests\n",
        "**Dur√©e estim√©e** : 2-3 jours\n",
        "**Composants** :\n",
        "- Interface React/Vue.js simple\n",
        "- Pr√©visualisation des s√©ances g√©n√©r√©es\n",
        "- Historique des requ√™tes\n",
        "- Feedback utilisateur\n",
        "\n",
        "### üöÄ PRIORIT√â 5 : MONITORING ET M√âTRIQUES\n",
        "**Objectif** : Observabilit√© en production\n",
        "**Dur√©e estim√©e** : 1-2 jours\n",
        "**Composants** :\n",
        "- M√©triques de performance (latence, throughput)\n",
        "- Tracking des taux de succ√®s par type de requ√™te\n",
        "- Alerting sur les erreurs\n",
        "- Dashboard de monitoring\n",
        "\n",
        "### üìà M√âTRIQUES DE SUCC√àS ACTUELLES\n",
        "- **Taux de r√©ussite des tests** : 100% (7/7 cas de test valid√©s)\n",
        "- **Confiance moyenne** : >0.8 sur les cas critiques\n",
        "- **Temps de r√©ponse** : <2 secondes pour le pipeline complet\n",
        "- **Couverture linguistique** : Fran√ßais familier + technique cycliste\n",
        "\n",
        "### üéØ OBJECTIFS BUSINESS √Ä 30 JOURS\n",
        "1. **API en production** avec 99.9% uptime\n",
        "2. **100+ s√©ances** dans le corpus enrichi\n",
        "3. **Interface utilisateur** fonctionnelle\n",
        "4. **M√©triques de production** op√©rationnelles\n",
        "5. **Documentation compl√®te** pour d√©veloppeurs\n",
        "\n",
        "**Status actuel : 60% du MVP technique accompli** üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
