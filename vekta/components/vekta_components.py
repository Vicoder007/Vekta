#!/usr/bin/env python3
"""
Composants RAG r√©els pour Vekta
Extraits des notebooks de d√©veloppement
"""

import re
import time
import os
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import logging
import numpy as np

# Import du module d'enrichissement contextuel
try:
    from .contextual_enrichment import ContextualEnrichmentPipeline
    CONTEXTUAL_ENRICHMENT_AVAILABLE = True
    print("‚úÖ Enrichissement contextuel disponible")
except ImportError as e:
    CONTEXTUAL_ENRICHMENT_AVAILABLE = False
    print(f"‚ö†Ô∏è Enrichissement contextuel non disponible: {e}")

# RAG et embeddings - Import conditionnel s√©curis√©
SENTENCE_TRANSFORMERS_AVAILABLE = False
try:
    # Test d'import s√©curis√©
    import sentence_transformers
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
    print("‚úÖ sentence-transformers disponible")
except (ImportError, ValueError, Exception) as e:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print(f"‚ö†Ô∏è  sentence-transformers non disponible ({type(e).__name__}), utilisation du mode basique")

logger = logging.getLogger(__name__)

# ================================
# CORRECTION ORTHOGRAPHIQUE
# ================================

class SpellChecker:
    """Correcteur orthographique sp√©cialis√© pour le cyclisme fran√ßais"""
    
    def __init__(self):
        # Vocabulaire cycliste fran√ßais
        self.cycling_vocabulary = {
            # Termes techniques
            'ftp', 'seuil', 'tempo', 'endurance', 'aerobic', 'anaerobie',
            'vo2max', 'puissance', 'watts', 'cadence', 'frequence',
            
            # Types d'entra√Ænement
            'intervals', 'series', 'repetitions', 'pyramide', 'over-under',
            'sweet-spot', 'threshold', 'recovery', 'base',
            
            # Dur√©es et mesures
            'minutes', 'secondes', 'heures', 'min', 'sec', 'h',
            
            # Intensit√©s
            'facile', 'modere', 'dur', 'max', 'maximum', 'fond',
            
            # Structure de s√©ance
            'echauffement', 'retour', 'calme', 'repos', 'recuperation',
            'pause', 'travail', 'effort'
        }
        
        # Corrections sp√©cifiques au cyclisme
        self.cycling_corrections = {
            # Phon√©tiques
            'aerobik': 'aerobic',
            'seuille': 'seuil',
            'recupe': 'recuperation',
            'recup': 'recuperation',
            'piramide': 'pyramide',
            'anaerobik': 'anaerobie',
            
            # Familier vers technique
            'chaude': 'echauffement',
            'chauffe': 'echauffement',
            'warm': 'echauffement',
            'cool': 'retour',
            'down': 'calme',
            'set': 'series',
            'sets': 'series',
            'rep': 'repetitions',
            'reps': 'repetitions',
            'pose': 'repos',
            'pause': 'repos',
            'break': 'repos',
            
            # Intensit√©s famili√®res
            'fond': 'max',
            'donf': 'max',
            'max': 'maximum',
            'facil': 'facile',
            'ezpz': 'facile',
            
            # Erreurs courantes
            'doie': 'dois',
            'doit': 'dois',
            'apres': 'apr√®s',
            'avk': 'avec',
            'avek': 'avec',
            'minut': 'minutes',
            'minuts': 'minutes',
            'mn': 'minutes',
            'fini': 'finir',
            'finit': 'finir',
        }
        
        # Expressions compos√©es
        self.compound_corrections = {
            'a fond': 'max',
            '√† fond': 'max',
            'cool down': 'retour au calme',
            'warm up': 'echauffement',
            'au max': 'maximum',
            'tr√®s dur': 'maximum',
            'super dur': 'maximum',
        }
    
    def correct_text(self, text: str) -> Tuple[str, List[str], float]:
        """
        Corrige le texte avec le vocabulaire cycliste
        Retourne: (texte_corrig√©, liste_corrections, confiance_correction)
        """
        original_text = text
        corrections = []
        
        # Normalisation
        text = text.lower().strip()
        
        # 1. Corrections compos√©es (phrases)
        for wrong, correct in self.compound_corrections.items():
            if wrong in text:
                text = text.replace(wrong, correct)
                corrections.append(f"'{wrong}' ‚Üí '{correct}'")
        
        # 2. Corrections mot par mot
        words = re.findall(r'\b\w+\b', text)
        corrected_words = []
        
        for word in words:
            if word in self.cycling_corrections:
                corrected_word = self.cycling_corrections[word]
                corrected_words.append(corrected_word)
                corrections.append(f"'{word}' ‚Üí '{corrected_word}'")
            else:
                # V√©rification par similarit√© (Levenshtein simple)
                best_match = self._find_best_match(word)
                if best_match and best_match != word:
                    corrected_words.append(best_match)
                    corrections.append(f"'{word}' ‚Üí '{best_match}'")
                else:
                    corrected_words.append(word)
        
        # Reconstruction du texte
        corrected_text = ' '.join(corrected_words)
        
        # Calcul de la confiance de correction
        correction_confidence = self._calculate_correction_confidence(
            original_text, corrected_text, len(corrections)
        )
        
        return corrected_text, corrections, correction_confidence
    
    def _find_best_match(self, word: str) -> Optional[str]:
        """Trouve la meilleure correspondance dans le vocabulaire"""
        if len(word) < 3:
            return None
        
        best_match = None
        best_score = 0
        
        for vocab_word in self.cycling_vocabulary:
            if len(vocab_word) >= 3:
                similarity = self._levenshtein_similarity(word, vocab_word)
                if similarity > 0.8 and similarity > best_score:
                    best_score = similarity
                    best_match = vocab_word
        
        return best_match if best_score > 0.8 else None
    
    def _levenshtein_similarity(self, s1: str, s2: str) -> float:
        """Calcule la similarit√© de Levenshtein normalis√©e"""
        if len(s1) == 0 or len(s2) == 0:
            return 0.0
        
        # Matrice de distance
        matrix = [[0] * (len(s2) + 1) for _ in range(len(s1) + 1)]
        
        for i in range(len(s1) + 1):
            matrix[i][0] = i
        for j in range(len(s2) + 1):
            matrix[0][j] = j
        
        for i in range(1, len(s1) + 1):
            for j in range(1, len(s2) + 1):
                cost = 0 if s1[i-1] == s2[j-1] else 1
                matrix[i][j] = min(
                    matrix[i-1][j] + 1,      # suppression
                    matrix[i][j-1] + 1,      # insertion
                    matrix[i-1][j-1] + cost  # substitution
                )
        
        distance = matrix[len(s1)][len(s2)]
        max_len = max(len(s1), len(s2))
        return 1 - (distance / max_len)
    
    def _calculate_correction_confidence(self, original: str, corrected: str, num_corrections: int) -> float:
        """Calcule la confiance de la correction"""
        if num_corrections == 0:
            return 1.0
        
        # Base de confiance selon le nombre de corrections
        base_confidence = max(0.5, 1.0 - (num_corrections * 0.1))
        
        # Bonus si les corrections sont coh√©rentes avec le vocabulaire cycliste
        corrected_words = set(re.findall(r'\b\w+\b', corrected.lower()))
        vocab_matches = len(corrected_words.intersection(self.cycling_vocabulary))
        vocab_bonus = min(0.3, vocab_matches * 0.05)
        
        return min(1.0, base_confidence + vocab_bonus)

# ================================
# CORPUS DE S√âANCES
# ================================

@dataclass
class WorkoutMetadata:
    """M√©tadonn√©es d'une s√©ance d'entra√Ænement"""
    name: str
    description: str
    duration_minutes: int
    difficulty: int  # 1-5
    zone: str
    complexity: str  # simple, complex, complete
    structure: List[str]  # ['warmup', 'main', 'cooldown']

class EnhancedCorpus:
    """Corpus enrichi de s√©ances d'entra√Ænement"""
    
    def __init__(self):
        self.workouts = self._initialize_corpus()
    
    def _initialize_corpus(self) -> List[Dict[str, Any]]:
        """Initialise le corpus avec des s√©ances vari√©es - CORPUS COMPLET"""
        return [
            # S√©ances compl√®tes VO2 MAX
            {
                'text': '10min echauffement puis 3 series de 5min VO2 max avec 2min repos entre series puis 10min retour au calme',
                'metadata': WorkoutMetadata(
                    name='3x5min VO2max',
                    description='S√©ance VO2max compl√®te avec √©chauffement et r√©cup√©ration',
                    duration_minutes=41,
                    difficulty=4,
                    zone='Zone 5',
                    complexity='complete',
                    structure=['warmup', 'main', 'cooldown']
                )
            },
            {
                'text': '10 minutes echauffements, 3 set de 5 mn VO2 max et 2 min pause entre set. 10 min cool down facile',
                'metadata': WorkoutMetadata(
                    name='3x5min VO2max Alternative',
                    description='S√©ance VO2max avec pause entre sets',
                    duration_minutes=41,
                    difficulty=4,
                    zone='Zone 5',
                    complexity='complete',
                    structure=['warmup', 'main', 'cooldown']
                )
            },
            {
                'text': '15min echauffement progressif puis 5x5min a 95% FTP avec 2min recuperation puis 15min retour calme',
                'metadata': WorkoutMetadata(
                    name='S√©ance VO2 Max 5x5min',
                    description='Travail de VO2 Max structur√©',
                    duration_minutes=65,
                    difficulty=5,
                    zone='Zone 5',
                    complexity='complete',
                    structure=['warmup', 'main', 'cooldown']
                )
            },
            {
                'text': '12min echauffement puis 4 fois 4min seuil avec 90sec repos puis 8min retour au calme',
                'metadata': WorkoutMetadata(
                    name='4x4min Seuil',
                    description='Intervalles seuil classiques',
                    duration_minutes=42,
                    difficulty=4,
                    zone='Zone 4',
                    complexity='complete',
                    structure=['warmup', 'main', 'cooldown']
                )
            },
            # S√©ances TEMPO/SEUIL
            {
                'text': '15 minutes echauffement progressif puis 20 minutes tempo seuil puis 10 minutes retour calme',
                'metadata': WorkoutMetadata(
                    name='Tempo 20min',
                    description='S√©ance tempo seuil avec √©chauffement progressif',
                    duration_minutes=45,
                    difficulty=3,
                    zone='Zone 4',
                    complexity='complete',
                    structure=['warmup', 'main', 'cooldown']
                )
            },
            {
                'text': '20min echauffement puis 2x20min a 95% seuil avec 5min recuperation puis 15min facile',
                'metadata': WorkoutMetadata(
                    name='Double Seuil 2x20min',
                    description='Travail de seuil en blocs longs',
                    duration_minutes=85,
                    difficulty=4,
                    zone='Zone 4',
                    complexity='complete',
                    structure=['warmup', 'main', 'cooldown']
                )
            },
            # S√©ances COMPLEXES
            {
                'text': 'echauffement 10min puis pyramide 1-2-3-4-3-2-1 minutes a seuil avec 1min recuperation entre puis retour calme',
                'metadata': WorkoutMetadata(
                    name='Pyramide Seuil',
                    description='S√©ance pyramide progressive au seuil',
                    duration_minutes=50,
                    difficulty=4,
                    zone='Zone 4',
                    complexity='complex',
                    structure=['warmup', 'main', 'cooldown']
                )
            },
            {
                'text': '6 fois 30sec max avec 30sec repos puis 5min facile puis 4 fois 2min tempo avec 1min repos',
                'metadata': WorkoutMetadata(
                    name='Mixed VO2+Tempo',
                    description='S√©ance mixte VO2max et tempo',
                    duration_minutes=35,
                    difficulty=5,
                    zone='Mixed',
                    complexity='complex',
                    structure=['main']
                )
            },
            {
                'text': '5 fois 3min over-under alternant 90sec a 95% et 90sec a 105% avec 2min repos',
                'metadata': WorkoutMetadata(
                    name='Over-Under 5x3min',
                    description='Intervalles over-under autour du seuil',
                    duration_minutes=23,
                    difficulty=4,
                    zone='Zone 4',
                    complexity='complex',
                    structure=['main']
                )
            },
            # S√©ances AEROBIE/ENDURANCE
            {
                'text': '45 minutes aerobic zone2',
                'metadata': WorkoutMetadata(
                    name='Aerobic 45min',
                    description='S√©ance a√©robie continue',
                    duration_minutes=45,
                    difficulty=2,
                    zone='Zone 2',
                    complexity='simple',
                    structure=['main']
                )
            },
            {
                'text': '30 minutes endurance',
                'metadata': WorkoutMetadata(
                    name='Endurance Base',
                    description='Sortie endurance de base',
                    duration_minutes=30,
                    difficulty=1,
                    zone='Zone 2',
                    complexity='simple',
                    structure=['main']
                )
            },
            # S√©ances SIMPLES
            {
                'text': '10 minutes de tempo',
                'metadata': WorkoutMetadata(
                    name='Tempo Simple',
                    description='Travail de tempo basique',
                    duration_minutes=10,
                    difficulty=2,
                    zone='Zone 3',
                    complexity='simple',
                    structure=['main']
                )
            },
            {
                'text': '8 fois 1min max avec 1min repos',
                'metadata': WorkoutMetadata(
                    name='8x1min VO2max',
                    description='Intervalles courts VO2max',
                    duration_minutes=16,
                    difficulty=4,
                    zone='Zone 5',
                    complexity='simple',
                    structure=['main']
                )
            },
        ]
    
    def search_similar(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Recherche les s√©ances similaires √† la requ√™te - VERSION AMELIOREE"""
        
        if SENTENCE_TRANSFORMERS_AVAILABLE and hasattr(self, 'embedding_model'):
            # Utiliser les embeddings vectoriels si disponibles
            return self._search_with_embeddings(query, max_results)
        else:
            # Fallback sur similarit√© lexicale am√©lior√©e
            return self._search_with_enhanced_lexical(query, max_results)
    
    def _search_with_enhanced_lexical(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """Recherche lexicale am√©lior√©e avec synonymes cyclistes"""
        
        # Synonymes pour am√©liorer la correspondance
        cycling_synonyms = {
            'vo2 max': ['max', 'fond', 'vo2max', 'vo2'],
            'vo2max': ['max', 'fond', 'vo2 max', 'vo2'],
            'max': ['vo2 max', 'vo2max', 'fond'],
            'fond': ['max', 'vo2 max', 'vo2max'],
            'seuil': ['tempo', 'threshold', 'ftp'],
            'tempo': ['seuil', 'threshold'],
            'echauffement': ['chauffe', 'warmup', 'warm'],
            'retour au calme': ['cool down', 'cooldown', 'retour calme'],
            'cool down': ['retour au calme', 'cooldown'],
            'series': ['set', 'fois', 'repetitions'],
            'set': ['series', 'fois'],
            'repos': ['pause', 'recuperation', 'recup'],
            'pause': ['repos', 'recuperation'],
        }
        
        # Expansion de la requ√™te avec synonymes
        expanded_query = query.lower()
        for term, synonyms in cycling_synonyms.items():
            if term in expanded_query:
                for synonym in synonyms:
                    expanded_query += f" {synonym}"
        
        query_words = set(re.findall(r'\b\w+\b', expanded_query))
        results = []
        
        for workout in self.workouts:
            # Expansion du texte de workout aussi
            expanded_workout_text = workout['text'].lower()
            for term, synonyms in cycling_synonyms.items():
                if term in expanded_workout_text:
                    for synonym in synonyms:
                        expanded_workout_text += f" {synonym}"
            
            workout_words = set(re.findall(r'\b\w+\b', expanded_workout_text))
            
            # Calcul de similarit√© Jaccard
            intersection = len(query_words.intersection(workout_words))
            union = len(query_words.union(workout_words))
            base_similarity = intersection / union if union > 0 else 0
            
            # Bonus pour correspondance exacte de termes cl√©s
            key_terms = ['vo2', 'max', 'seuil', 'tempo', 'echauffement', 'series', 'set']
            key_matches = sum(1 for term in key_terms if term in query.lower() and term in workout['text'].lower())
            key_bonus = key_matches * 0.15
            
            # Bonus pour structure compl√®te
            structure_bonus = 0.1 if workout['metadata'].complexity == 'complete' else 0
            
            # Bonus pour dur√©e similaire (si d√©tectable dans la requ√™te)
            duration_bonus = 0.05 if self._has_similar_duration(query, workout['metadata'].duration_minutes) else 0
            
            final_similarity = min(1.0, base_similarity + key_bonus + structure_bonus + duration_bonus)
            
            results.append({
                **workout,
                'similarity': final_similarity
            })
        
        # Tri par similarit√© d√©croissante
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:max_results]
    
    def _search_with_embeddings(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """Recherche avec embeddings vectoriels (si sentence-transformers disponible)"""
        
        if not hasattr(self, 'embedding_model'):
            self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
            # Encoder tous les textes du corpus
            corpus_texts = [w['text'] for w in self.workouts]
            self.corpus_embeddings = self.embedding_model.encode(corpus_texts, convert_to_tensor=True)
        
        # Encoder la requ√™te
        query_embedding = self.embedding_model.encode([query], convert_to_tensor=True)
        
        # Calcul des similarit√©s cosinus
        from sentence_transformers.util import pytorch_cos_sim
        similarities = pytorch_cos_sim(query_embedding, self.corpus_embeddings)[0]
        
        # Cr√©er les r√©sultats avec scores
        results = []
        for i, workout in enumerate(self.workouts):
            similarity_score = float(similarities[i])
            
            # Bonus pour structure compl√®te
            if workout['metadata'].complexity == 'complete':
                similarity_score += 0.05
            
            results.append({
                **workout,
                'similarity': similarity_score
            })
        
        # Tri par similarit√© d√©croissante
        results.sort(key=lambda x: x['similarity'], reverse=True)
        return results[:max_results]
    
    def _has_similar_duration(self, query: str, target_duration: int) -> bool:
        """D√©tecte si la requ√™te mentionne une dur√©e similaire"""
        import re
        duration_patterns = re.findall(r'(\d+)\s*(?:min|minute)', query.lower())
        if duration_patterns:
            query_duration = sum(int(d) for d in duration_patterns)
            return abs(query_duration - target_duration) <= 10
        return False

# ================================
# PARSEUR STRUCTUREL VEKTA
# ================================

class VektaStructuralParser:
    """
    Parseur structurel reproduisant la logique Vekta r√©elle
    Extraction num√©rique pr√©cise et classification des structures d'entra√Ænement
    """
    
    def __init__(self):
        # Patterns num√©riques ultra-pr√©cis (niveau Vekta)
        self.precision_patterns = {
            'duration_precise': r'(\d+)min(\d+)s',                    # 4min33s
            'duration_minute': r'(\d+)(?:\s*min|minute)s?',          # 10min, 15 minutes
            'duration_hour': r'(\d+)h(\d+)(?:min)?',                 # 2h30, 3h15min
            'intensity_decimal': r'(\d+\.?\d*)%(?:\s*ftp)?',         # 87.3%FTP, 95%
            'intensity_range': r'(\d+)%.*?(\d+)%',                   # entre 103% et 91%
            'repetitions': r'(\d+)x',                                # 13x, 5x
            'alternation': r'(\d+)s.*?entre.*?(\d+)%.*?(\d+)%.*?(\d+).*?min',  # 47s entre 103% et 91% pendant 23min
            'fibonacci': r'(?:fibonacci|fibo).*?(\d+-\d+-\d+-\d+-\d+)',        # fibonacci 1-1-2-3-5-8
            'progression': r'(\d+)-(\d+)-(\d+)-(\d+)',               # 1-2-3-4min
            'spiral': r'(\d+)min.*?(\d+)%.*?(\d+)min.*?(\d+)%.*?(\d+)min.*?(\d+)%.*?redescendre',  # spirale
        }
        
        # Zones d'intensit√© Vekta (mapping exact observ√©)
        self.vekta_zones = {
            'recovery': (0, 55, 'recovery'),
            'aerobic': (55, 75, 'aerobic'), 
            'tempo': (75, 90, 'tempo'),
            'threshold': (90, 105, 'threshold'),
            'vo2max': (105, 130, 'vo2max'),
            'anaerobic': (130, 180, 'anaerobic'),
            'neuromuscular': (180, 500, 'neuromuscular')
        }
        
        # Structures connues (bas√©es sur tests observ√©s)
        self.structure_patterns = {
            'simple_intervals': r'(\d+)x.*?(\d+)min.*?(\d+)%.*?(\d+)min.*?r[e√©]cup',
            'alternation': r'alternance|altern',
            'progression': r'progression|progress',
            'fibonacci': r'fibonacci|fibo',
            'spiral': r'spiral|spirale',
            'pyramid': r'pyramid|pyramide',
            'over_under': r'over.*?under',
            'continuous': r'continu|continue'
        }
    
    def parse_query(self, query: str) -> Dict[str, Any]:
        """
        Parse principal reproduisant la logique Vekta
        Retourne param√®tres structur√©s avec score de compl√©tude
        """
        query_lower = query.lower()
        
        # Extraction num√©rique pr√©cise
        durations = self._extract_precise_durations(query_lower)
        intensities = self._extract_precise_intensities(query_lower)
        repetitions = self._extract_repetitions(query_lower)
        structure = self._classify_structure_type(query_lower)
        
        # Calcul automatique pour structures complexes
        calculated_values = self._perform_vekta_calculations(query_lower, durations, intensities, repetitions)
        
        # Score de compl√©tude (logique Vekta)
        completeness = self._assess_vekta_completeness(durations, intensities, repetitions, structure)
        
        return {
            'durations': durations,
            'intensities': intensities,
            'repetitions': repetitions,
            'structure_type': structure,
            'calculated_values': calculated_values,
            'completeness_score': completeness,
            'is_coach_suitable': completeness >= 0.6,  # Seuil coach mode
            'is_auto_generate': completeness >= 0.9,   # Seuil g√©n√©ration auto
            'missing_critical': self._identify_missing_critical(durations, intensities, repetitions) if completeness < 0.9 else []
        }
    
    def _extract_precise_durations(self, query: str) -> Dict[str, Any]:
        """Extraction dur√©es avec pr√©cision Vekta (¬±1s)"""
        durations = {
            'total_minutes': 0,
            'work_intervals': [],
            'recovery_intervals': [],
            'warmup': 0,
            'cooldown': 0,
            'precision_level': 'none'
        }
        
        # Dur√©es pr√©cises au niveau seconde (4min33s)
        precise_matches = re.findall(self.precision_patterns['duration_precise'], query)
        if precise_matches:
            for min_val, sec_val in precise_matches:
                total_seconds = int(min_val) * 60 + int(sec_val)
                durations['work_intervals'].append(total_seconds)
                durations['precision_level'] = 'second'
        
        # Dur√©es standard (10min, 15 minutes)
        minute_matches = re.findall(self.precision_patterns['duration_minute'], query)
        if minute_matches:
            for min_val in minute_matches:
                durations['work_intervals'].append(int(min_val) * 60)
                if durations['precision_level'] == 'none':
                    durations['precision_level'] = 'minute'
        
        # R√©cup√©rations
        recovery_match = re.search(r'(\d+)min(\d+)?s?.*?(?:r[e√©]cup|repos|pause)', query)
        if recovery_match:
            min_val = int(recovery_match.group(1))
            sec_val = int(recovery_match.group(2)) if recovery_match.group(2) else 0
            durations['recovery_intervals'].append(min_val * 60 + sec_val)
        
        # √âchauffement
        warmup_match = re.search(r'(\d+)min.*?(?:[e√©]chauffement|warm|chauffe)', query)
        if warmup_match:
            durations['warmup'] = int(warmup_match.group(1)) * 60
        
        # Retour au calme
        cooldown_match = re.search(r'(\d+)min.*?(?:retour|cool|calme)', query)
        if cooldown_match:
            durations['cooldown'] = int(cooldown_match.group(1)) * 60
        
        return durations
    
    def _extract_precise_intensities(self, query: str) -> Dict[str, Any]:
        """Extraction intensit√©s avec pr√©cision d√©cimale Vekta"""
        intensities = {
            'primary': None,
            'secondary': None,
            'range': None,
            'zones_detected': [],
            'precision_level': 'none'
        }
        
        # Intensit√©s d√©cimales pr√©cises (87.3%FTP)
        decimal_matches = re.findall(self.precision_patterns['intensity_decimal'], query)
        if decimal_matches:
            intensities['primary'] = float(decimal_matches[0])
            intensities['precision_level'] = 'decimal'
            if len(decimal_matches) > 1:
                intensities['secondary'] = float(decimal_matches[1])
        
        # Ranges d'intensit√© (103% et 91%)
        range_matches = re.findall(self.precision_patterns['intensity_range'], query)
        if range_matches:
            intensities['range'] = [float(range_matches[0][0]), float(range_matches[0][1])]
        
        # Zones textuelles
        for zone_name, (min_pct, max_pct, zone_key) in self.vekta_zones.items():
            if zone_name in query or zone_key in query:
                intensities['zones_detected'].append(zone_key)
        
        return intensities
    
    def _extract_repetitions(self, query: str) -> Dict[str, Any]:
        """Extraction r√©p√©titions et structures"""
        repetitions = {
            'primary': None,
            'secondary': None,
            'structure': 'simple'
        }
        
        # R√©p√©titions simples (13x)
        rep_matches = re.findall(self.precision_patterns['repetitions'], query)
        if rep_matches:
            repetitions['primary'] = int(rep_matches[0])
            if len(rep_matches) > 1:
                repetitions['secondary'] = int(rep_matches[1])
                repetitions['structure'] = 'nested'
        
        return repetitions
    
    def _classify_structure_type(self, query: str) -> str:
        """Classification type de structure (logique Vekta)"""
        for structure_name, pattern in self.structure_patterns.items():
            if re.search(pattern, query):
                return structure_name
        
        # Fibonacci sp√©cial
        if re.search(r'fibonacci|fibo', query):
            return 'fibonacci'
        
        # Par d√©faut
        return 'simple_intervals' if re.search(r'\d+x', query) else 'continuous'
    
    def _perform_vekta_calculations(self, query: str, durations: Dict, intensities: Dict, repetitions: Dict) -> Dict[str, Any]:
        """Calculs automatiques √† la mani√®re Vekta"""
        calculations = {}
        
        # Alternances : calcul automatique nombre r√©p√©titions
        alternation_match = re.search(r'(\d+)s.*?entre.*?(\d+)%.*?(\d+)%.*?pendant.*?(\d+).*?min', query)
        if alternation_match:
            interval_seconds = int(alternation_match.group(1))
            total_minutes = int(alternation_match.group(4))
            total_seconds = total_minutes * 60
            num_intervals = total_seconds // (interval_seconds * 2)  # Alternance = 2 intervalles
            
            calculations['alternation'] = {
                'interval_duration': interval_seconds,
                'total_duration': total_seconds,
                'calculated_repetitions': num_intervals,
                'intensity_high': float(alternation_match.group(2)),
                'intensity_low': float(alternation_match.group(3))
            }
        
        # Progressions Fibonacci
        if 'fibonacci' in query:
            fibonacci_base = [1, 1, 2, 3, 5, 8]  # S√©quence standard
            calculations['fibonacci'] = {
                'sequence': fibonacci_base,
                'total_duration': sum(fibonacci_base) * 60,  # En secondes
                'progression_type': 'ascending'
            }
        
        return calculations
    
    def _assess_vekta_completeness(self, durations: Dict, intensities: Dict, repetitions: Dict, structure: str) -> float:
        """Score compl√©tude selon logique Vekta"""
        score = 0.0
        
        # Dur√©e pr√©sente (critique)
        if durations['work_intervals'] or durations['total_minutes'] > 0:
            score += 0.4
            # Bonus pr√©cision
            if durations['precision_level'] == 'second':
                score += 0.1
        
        # Intensit√© pr√©sente (critique)
        if intensities['primary'] or intensities['zones_detected']:
            score += 0.4
            # Bonus pr√©cision d√©cimale
            if intensities['precision_level'] == 'decimal':
                score += 0.1
        
        # Structure identifi√©e
        if structure != 'continuous':
            score += 0.1
        
        # R√©p√©titions pour intervalles
        if repetitions['primary'] and structure in ['simple_intervals', 'nested']:
            score += 0.1
        
        return min(1.0, score)
    
    def _identify_missing_critical(self, durations: Dict, intensities: Dict, repetitions: Dict) -> List[str]:
        """Identifie informations critiques manquantes (messages Vekta-style)"""
        missing = []
        
        if not durations['work_intervals'] and durations['total_minutes'] == 0:
            missing.append("1) The total duration of the session")
        
        if not intensities['primary'] and not intensities['zones_detected']:
            missing.append("2) The intensity targets for each effort (power zones, specific power values, or RPE)")
        
        if repetitions['primary'] and not durations['recovery_intervals']:
            missing.append("3) Recovery periods between intervals")
        
        return missing

# ================================
# PIPELINE RAG COMPLET
# ================================

class RAGPipeline:
    """
    Pipeline Vekta Hybride - Architecture R√©elle Reproduite
    
    Architecture Hybride:
    1. Parseur structurel PRIMAIRE (90% des cas) - Pr√©cision Vekta
    2. Corpus RAG SECONDAIRE (10% des cas) - Validation/Fallback
    3. Mode Coach = Zero validation physiologique
    4. Mode User = Validation corpus + r√®gles
    5. G√©n√©ration .zwo fid√®le production
    """
    
    def __init__(self):
        # Composants principaux (ordre d'importance Vekta)
        self.structural_parser = VektaStructuralParser()  # PRIMAIRE
        self.spell_checker = SpellChecker()
        self.corpus = EnhancedCorpus()                    # SECONDAIRE
        
        # Enrichissement contextuel
        if CONTEXTUAL_ENRICHMENT_AVAILABLE:
            self.contextual_enrichment = ContextualEnrichmentPipeline()
        else:
            self.contextual_enrichment = None
        
        # Seuils Vekta r√©els (bas√©s sur tests observ√©s)
        self.vekta_auto_threshold = 0.9    # G√©n√©ration automatique (comme observ√©)
        self.vekta_coach_threshold = 0.6   # Seuil minimum mode coach
        self.vekta_user_threshold = 0.7    # Seuil mode utilisateur avec corpus
    
    def validate_query(self, query: str) -> Dict[str, Any]:
        """
        Validation Vekta Hybride - Parseur Structurel PRIMAIRE
        Reproduit exactement la logique observ√©e en production
        """
        start_time = time.time()
        
        try:
            # 1. Correction orthographique (NLU Vekta)
            corrected_query, corrections, correction_confidence = self.spell_checker.correct_text(query)
            
            # 2. PARSING STRUCTUREL PRIMAIRE (Logique Vekta dominante)
            parsed_params = self.structural_parser.parse_query(corrected_query)
            
            # 3. D√©cision selon architecture Vekta r√©elle
            if parsed_params['is_auto_generate']:
                # CAS 1: G√©n√©ration automatique (>90% compl√©tude)
                return self._generate_from_structural_params(
                    parsed_params, corrected_query, corrections, start_time
                )
            
            elif parsed_params['is_coach_suitable']:
                # CAS 2: Mode "Open Duration" avec corpus validation
                return self._generate_with_corpus_enrichment(
                    parsed_params, corrected_query, corrections, start_time
                )
            
            else:
                # CAS 3: Informations critiques manquantes (comme Vekta)
                return self._request_missing_information_vekta_style(
                    parsed_params, corrected_query, corrections, start_time
                )
            
        except Exception as e:
            logger.error(f"Erreur dans validate_query: {e}")
            return {
                'success': False,
                'confidence': 0.0,
                'message': f"Erreur de traitement: {str(e)}",
                'status': 'error',
                'workout': None,
                'correction_applied': False,
                'corrections': [],
                'processing_time': time.time() - start_time
            }
    
    def _generate_from_structural_params(self, parsed_params: Dict[str, Any], corrected_query: str, 
                                       corrections: List[str], start_time: float) -> Dict[str, Any]:
        """
        G√©n√©ration directe bas√©e sur parsing structurel (>90% compl√©tude)
        Reproduit le comportement Vekta haute confiance
        """
        # G√©n√©ration param√©trique directe (mode Vekta confiance 95%)
        try:
            extractor = AdvancedParameterExtractor()
            generator = AdvancedParametricGenerator(extractor)
            
            workout = generator.generate_advanced_workout(corrected_query, ftp_watts=250)
            
            # Enrichissement avec donn√©es parsing structurel
            confidence_score = parsed_params['completeness_score']
            
            return {
                'success': True,
                'confidence': 0.95,  # Confiance Vekta haute
                'message': f"‚úÖ S√©ance g√©n√©r√©e automatiquement (pr√©cision: {confidence_score:.1%})",
                'status': 'generated_vekta_precision',
                'workout': {
                    'name': workout.name,
                    'description': workout.description,
                    'duration_minutes': workout.total_duration,
                    'difficulty': workout.difficulty,
                    'training_load': workout.training_load,
                    'segments': [
                        {
                            'name': s.name,
                            'duration': s.duration_minutes,
                            'intensity_start': s.intensity_percent_start,
                            'zone': s.zone,
                            'type': s.segment_type
                        }
                        for s in workout.segments
                    ],
                    'vekta_features': {
                        'structural_parsing': True,
                        'precision_level': parsed_params['durations']['precision_level'],
                        'structure_type': parsed_params['structure_type'],
                        'calculated_values': parsed_params['calculated_values']
                    }
                },
                'correction_applied': len(corrections) > 0,
                'corrections': corrections,
                'processing_time': time.time() - start_time
            }
        except Exception as e:
            # Fallback vers corpus si g√©n√©ration param√©trique √©choue
            return self._generate_with_corpus_enrichment(parsed_params, corrected_query, corrections, start_time)
    
    def _generate_with_corpus_enrichment(self, parsed_params: Dict[str, Any], corrected_query: str,
                                       corrections: List[str], start_time: float) -> Dict[str, Any]:
        """
        Mode "Open Duration" avec validation corpus (logique Vekta 60-90% compl√©tude)
        """
        # Recherche dans corpus pour enrichissement
        similar_workouts = self.corpus.search_similar(corrected_query, max_results=3)
        
        if similar_workouts:
            best_match = similar_workouts[0]
            corpus_confidence = best_match['similarity']
            
            # Score hybride parsing + corpus (logique Vekta)
            hybrid_confidence = min(0.85, (parsed_params['completeness_score'] + corpus_confidence) / 2)
            
            # Enrichissement avec donn√©es parsing structurel
            enriched_workout = {
                'text': best_match['text'],
                'metadata': {
                    'name': best_match['metadata'].name,
                    'description': best_match['metadata'].description,
                    'duration_minutes': best_match['metadata'].duration_minutes,
                    'difficulty': best_match['metadata'].difficulty,
                    'zone': best_match['metadata'].zone
                },
                'vekta_enrichment': {
                    'structural_params': parsed_params,
                    'corpus_similarity': corpus_confidence,
                    'precision_level': parsed_params['durations']['precision_level'],
                    'open_duration_applied': True  # Mode Vekta Open Duration
                }
            }
            
            return {
                'success': True,
                'confidence': hybrid_confidence,
                'message': f"‚ö†Ô∏è S√©ance g√©n√©r√©e avec enrichissement corpus ({hybrid_confidence:.1%})\nüí° 'Open duration' appliqu√©e aux √©l√©ments non sp√©cifi√©s",
                'status': 'generated_with_corpus_enrichment',
                'workout': enriched_workout,
                'correction_applied': len(corrections) > 0,
                'corrections': corrections,
                'processing_time': time.time() - start_time
            }
        else:
            # Pas de match corpus - tentative g√©n√©ration param√©trique quand m√™me
            return self._generate_from_structural_params(parsed_params, corrected_query, corrections, start_time)
    
    def _request_missing_information_vekta_style(self, parsed_params: Dict[str, Any], corrected_query: str,
                                               corrections: List[str], start_time: float) -> Dict[str, Any]:
        """
        Demande d'informations manquantes - style Vekta exact
        Messages d'erreur reproductibles
        """
        missing_info = parsed_params['missing_critical']
        completeness = parsed_params['completeness_score']
        
        # Messages style Vekta (observ√©s en production)
        if not missing_info:
            # Cas g√©n√©ral - informations insuffisantes
            message = f"The workout description is missing required information. Please specify: 1) The total duration of the session, 2) The specific workout structure (warm-up, intervals, recovery periods), and 3) The intensity targets for each segment (power zones, specific power values, or RPE)."
        else:
            # Messages sp√©cifiques bas√©s sur analyse structurelle
            message = f"The workout description is missing required information. Please specify: {', '.join(missing_info)}"
        
        return {
            'success': False,
            'confidence': completeness,
            'message': message,
            'status': 'missing_critical_info_vekta',
            'workout': None,
            'missing_elements': missing_info,
            'structural_analysis': {
                'completeness_score': completeness,
                'durations_detected': bool(parsed_params['durations']['work_intervals']),
                'intensities_detected': bool(parsed_params['intensities']['primary']),
                'structure_detected': parsed_params['structure_type']
            },
            'correction_applied': len(corrections) > 0,
            'corrections': corrections,
            'processing_time': time.time() - start_time
        }
    
    def _analyze_suspicious_elements(self, query: str, best_match: Dict[str, Any]) -> List[str]:
        """Analyse les √©l√©ments suspects dans une requ√™te (confiance 75-90%)"""
        suspicious = []
        query_lower = query.lower()
        
        # Dur√©es suspectes ou manquantes
        duration_matches = re.findall(r'(\d+)\s*(?:min|minute)', query_lower)
        if not duration_matches:
            suspicious.append("dur√©e non sp√©cifi√©e")
        elif any(int(d) > 120 for d in duration_matches):
            suspicious.append("dur√©e tr√®s longue")
        
        # Intensit√©s vagues
        vague_intensities = ['dur', 'facile', 'moyen', 'normale']
        if any(term in query_lower for term in vague_intensities):
            suspicious.append("intensit√© vague")
        
        # Structure incompl√®te
        has_warmup = any(term in query_lower for term in ['echauffement', 'chauffe', 'warm'])
        has_cooldown = any(term in query_lower for term in ['retour', 'cool', 'calme'])
        if not has_warmup:
            suspicious.append("pas d'√©chauffement")
        if not has_cooldown:
            suspicious.append("pas de retour au calme")
        
        # Termes ambigus
        ambiguous_terms = ['set', 'fois', 'apr√®s', 'puis']
        ambiguous_count = sum(1 for term in ambiguous_terms if term in query_lower)
        if ambiguous_count > 2:
            suspicious.append("formulation ambigu√´")
        
        return suspicious[:3]  # Max 3 √©l√©ments
    
    def _detect_missing_for_defaults(self, query: str) -> List[str]:
        """
        D√©tecte ce qui sera remplac√© par des valeurs par d√©faut (comme production)
        Production utilise: "Open duration" + type aerobic par d√©faut
        """
        defaults_applied = []
        query_lower = query.lower()
        
        # R√©cup√©rations non sp√©cifi√©es ‚Üí "Open duration"
        has_intervals = re.search(r'(\d+)\s*(?:fois|set|serie)', query_lower)
        has_recovery = re.search(r'(?:repos|pause|recuperation|recup)', query_lower)
        if has_intervals and not has_recovery:
            defaults_applied.append("'Open duration' pour r√©cup√©rations")
        
        # Type d'intensit√© vague ‚Üí aerobic par d√©faut
        has_specific_intensity = any(term in query_lower for term in [
            'vo2', 'max', 'seuil', 'tempo', 'threshold', '%'
        ])
        if not has_specific_intensity:
            defaults_applied.append("type 'aerobic' par d√©faut")
        
        # √âchauffement/cooldown manquants mais structure d√©tect√©e
        has_main_work = re.search(r'\d+.*(?:min|fois)', query_lower)
        has_warmup = any(term in query_lower for term in ['echauffement', 'chauffe', 'warm'])
        has_cooldown = any(term in query_lower for term in ['retour', 'cool', 'calme'])
        
        if has_main_work:
            if not has_warmup:
                defaults_applied.append("√©chauffement standard")
            if not has_cooldown:
                defaults_applied.append("retour au calme standard")
        
        return defaults_applied
    
    def _detect_critical_missing_entities(self, query: str) -> List[str]:
        """
        D√©tecte les informations critiques manquantes (√©chec comme production)
        Production √©choue sur: pas de dur√©e principale OU pas de structure identifiable
        """
        critical_missing = []
        query_lower = query.lower()
        
        # Aucune dur√©e d√©tect√©e = critique
        if not re.search(r'\d+', query_lower):
            critical_missing.append("dur√©es (ex: '5 minutes', '10min')")
        
        # Aucune structure d'entra√Ænement = critique
        has_structure = any(term in query_lower for term in [
            'fois', 'set', 'serie', 'repetition', 'interval', 'min'
        ])
        if not has_structure:
            critical_missing.append("structure d'entra√Ænement (ex: '3 fois 5 minutes')")
        
        # Trop vague = critique (moins de 3 mots significatifs)
        significant_words = [w for w in query_lower.split() if len(w) > 2]
        if len(significant_words) < 3:
            critical_missing.append("description d√©taill√©e de la s√©ance")
        
        # Aucun indicateur d'intensit√© ni de zone = critique
        has_any_intensity = any(term in query_lower for term in [
            'vo2', 'max', 'seuil', 'tempo', 'aerobic', 'endurance', 'facile', 'dur', '%', 'ftp'
        ])
        if not has_any_intensity:
            critical_missing.append("intensit√© ou zone (ex: 'VO2max', 'seuil', 'facile')")
        
        return critical_missing[:3]  # Max 3 pour lisibilit√©
    
    def generate_parametric_workout(self, query: str, ftp_watts: int = 250, use_advanced: bool = True) -> Dict[str, Any]:
        """
        G√©n√®re une s√©ance param√©trique pour coachs experts (mode sans validation)
        
        Args:
            query: Requ√™te utilisateur
            ftp_watts: FTP en watts pour le calcul des intensit√©s
            use_advanced: Utilise le g√©n√©rateur avanc√© si True, sinon version simple
        
        Returns:
            Dictionnaire avec la s√©ance g√©n√©r√©e et les m√©tadonn√©es
        """
        start_time = time.time()
        
        try:
            # 1. Correction orthographique
            corrected_text, corrections, correction_confidence = self.spell_checker.correct_text(query)
            
            # 2. Choix du g√©n√©rateur
            if use_advanced:
                # G√©n√©rateur avanc√© pour production
                extractor = AdvancedParameterExtractor()
                generator = AdvancedParametricGenerator(extractor)
                zwo_gen = AdvancedZwoGenerator()
                
                # G√©n√©ration avanc√©e
                workout = generator.generate_advanced_workout(corrected_text, ftp_watts)
                
                # Cr√©ation du fichier .zwo
                zwo_file = zwo_gen.create_advanced_zwo(workout, ftp_watts)
                
                result = {
                    'success': True,
                    'confidence': 0.95,  # Haute confiance en mode param√©trique
                    'mode': 'parametric_advanced',
                    'message': f"S√©ance g√©n√©r√©e param√©triquement (TSS: {workout.training_load})",
                    'workout': {
                        'name': workout.name,
                        'description': workout.description,
                        'duration_minutes': workout.total_duration,
                        'difficulty': workout.difficulty,
                        'workout_type': workout.workout_type,
                        'training_load': workout.training_load,
                        'target_systems': workout.target_systems,
                        'periodization_phase': workout.periodization_phase,
                        'segments': [
                            {
                                'name': s.name,
                                'duration': s.duration_minutes,
                                'intensity_start': s.intensity_percent_start,
                                'intensity_end': s.intensity_percent_end,
                                'zone': s.zone,
                                'type': s.segment_type,
                                'description': s.description,
                                'cadence': s.cadence_target,
                                'power_curve': s.power_curve
                            }
                            for s in workout.segments
                        ]
                    },
                    'zwo_file': zwo_file,
                    'correction_applied': len(corrections) > 0,
                    'corrections': corrections,
                    'features': {
                        'tss_calculation': True,
                        'advanced_zones': True,
                        'cadence_targets': True,
                        'power_curves': True,
                        'periodization_aware': True
                    }
                }
            else:
                # Version simple (fallback)
                # Importation locale pour √©viter les d√©pendances circulaires
                from typing import Dict, Any
                
                # Simulation d'un g√©n√©rateur simple (pourrait √™tre impl√©ment√©)
                result = {
                    'success': True,
                    'confidence': 0.85,
                    'mode': 'parametric_simple',
                    'message': "S√©ance g√©n√©r√©e avec g√©n√©rateur simple",
                    'workout': None,
                    'zwo_file': None,
                    'correction_applied': len(corrections) > 0,
                    'corrections': corrections
                }
            
            processing_time = time.time() - start_time
            result['processing_time'] = processing_time
            
            logger.info(f"G√©n√©ration param√©trique r√©ussie en {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"Erreur dans generate_parametric_workout: {e}")
            return {
                'success': False,
                'confidence': 0.0,
                'mode': 'parametric_error',
                'message': f"Erreur de g√©n√©ration param√©trique: {str(e)}",
                'workout': None,
                'zwo_file': None,
                'correction_applied': False,
                'corrections': [],
                'processing_time': time.time() - start_time
            }
    
    def hybrid_process(self, query: str, coach_mode: bool = False, ftp_watts: int = 250) -> Dict[str, Any]:
        """
        Pipeline Vekta Hybride Complet
        Mode Coach = Zero validation physiologique (95% confiance)
        Mode User = Validation hybride parsing + corpus
        """
        start_time = time.time()
        
        # Correction orthographique commune
        corrected_query, corrections, correction_confidence = self.spell_checker.correct_text(query)
        
        if coach_mode:
            # MODE COACH EXPERT - Vekta style (z√©ro validation physiologique)
            logger.info("üî• Mode coach expert - G√©n√©ration param√©trique sans validation")
            
            # Parsing structurel pour coach (accepte tout >60% compl√©tude)
            parsed_params = self.structural_parser.parse_query(corrected_query)
            
            if parsed_params['completeness_score'] >= self.vekta_coach_threshold:
                # G√©n√©ration param√©trique directe - ACCEPTE TOUT en mode coach
                try:
                    result = self.generate_parametric_workout(query, ftp_watts, use_advanced=True)
                    # Force confiance 95% en mode coach (comportement Vekta observ√©)
                    result['confidence'] = 0.95
                    result['mode'] = 'coach_expert_vekta'
                    result['message'] = f"üî• Coach Expert: S√©ance g√©n√©r√©e sans validation physiologique (TSS: {result['workout']['training_load']})"
                    result['vekta_coach_features'] = {
                        'zero_physiological_validation': True,
                        'accepts_extreme_values': True,  # 6h √† 130%FTP accept√©
                        'structural_parsing_used': True,
                        'corpus_bypassed': True
                    }
                    return result
                except Exception as e:
                    # M√™me en cas d'erreur, mode coach essaie de g√©n√©rer quelque chose
                    return {
                        'success': True,  # Mode coach g√©n√®re toujours quelque chose
                        'confidence': 0.85,
                        'mode': 'coach_expert_fallback',
                        'message': f"üî• Coach Expert: S√©ance g√©n√©r√©e avec param√®tres estim√©s",
                        'workout': {'name': 'S√©ance Coach Expert', 'description': corrected_query},
                        'processing_time': time.time() - start_time
                    }
            else:
                # M√™me avec compl√©tude faible, mode coach g√©n√®re (comportement Vekta)
                return {
                    'success': True,
                    'confidence': 0.85,
                    'mode': 'coach_expert_minimal',
                    'message': f"üî• Coach Expert: S√©ance g√©n√©r√©e avec interpr√©tation cr√©ative",
                    'workout': {'name': 'S√©ance Coach Expert', 'description': corrected_query},
                    'processing_time': time.time() - start_time
                }
        
        else:
            # MODE UTILISATEUR - Pipeline hybride avec validation
            logger.info("üë§ Mode utilisateur - Pipeline hybride avec validation")
            return self.validate_query(query)
    
    def process_with_contextual_enrichment(
        self, 
        query: str, 
        location: str = "Paris, France",
        athlete_fatigue: int = 3,
        sleep_hours: float = 7.5,
        stress_level: int = 3,
        indoor_outdoor: str = "flexible",
        equipment: List[str] = None,
        ftp_watts: int = 250
    ) -> Dict[str, Any]:
        """
        Pipeline complet avec enrichissement contextuel intelligent
        Combine g√©n√©ration Vekta + recommandations contextuelles
        """
        start_time = time.time()
        
        try:
            # 1. G√©n√©ration du workout de base (pipeline Vekta standard)
            base_result = self.hybrid_process(query, coach_mode=False, ftp_watts=ftp_watts)
            
            # 2. Enrichissement contextuel si disponible
            contextual_data = None
            if self.contextual_enrichment:
                contextual_data = self.contextual_enrichment.enrich_workout_request(
                    workout_query=query,
                    location=location,
                    athlete_fatigue=athlete_fatigue,
                    sleep_hours=sleep_hours,
                    stress_level=stress_level,
                    indoor_outdoor=indoor_outdoor,
                    equipment=equipment or ['trainer']
                )
            
            # 3. Combinaison des r√©sultats
            enriched_result = {
                **base_result,
                'contextual_enrichment': contextual_data,
                'enrichment_available': self.contextual_enrichment is not None,
                'total_processing_time': time.time() - start_time
            }
            
            # 4. Application des recommandations critiques automatiquement
            if contextual_data and contextual_data.get('success', False):
                critical_recommendations = [
                    rec for rec in contextual_data['recommendations'] 
                    if rec['priority'] == 'critical'
                ]
                
                if critical_recommendations:
                    enriched_result['critical_adaptations'] = []
                    for rec in critical_recommendations:
                        enriched_result['critical_adaptations'].append({
                            'adaptation': rec['recommended'],
                            'reason': rec['reason'],
                            'confidence': rec['confidence']
                        })
                    
                    # Mise √† jour du message principal
                    if base_result.get('success', False):
                        original_message = base_result.get('message', '')
                        adaptation_notes = '; '.join([rec['adaptation'] for rec in critical_recommendations])
                        enriched_result['message'] = f"{original_message}\nüå°Ô∏è Adaptations critiques: {adaptation_notes}"
            
            return enriched_result
            
        except Exception as e:
            logger.error(f"Erreur enrichissement contextuel: {e}")
            # Retour au pipeline standard en cas d'erreur
            base_result = self.hybrid_process(query, coach_mode=False, ftp_watts=ftp_watts)
            base_result['contextual_enrichment'] = {
                'success': False,
                'error': str(e)
            }
            base_result['enrichment_available'] = False
            return base_result

# ================================
# G√âN√âRATEUR ZWO
# ================================

class ZwoGenerator:
    """G√©n√©rateur de fichiers .zwo pour Zwift"""
    
    def create_zwo_file(self, workout_text: str, metadata: Dict[str, Any], output_dir: str = "./generated_workouts") -> str:
        """Cr√©e un fichier .zwo bas√© sur la s√©ance"""
        os.makedirs(output_dir, exist_ok=True)
        
        # Nom de fichier unique
        timestamp = int(time.time())
        filename = f"vekta_workout_{timestamp}.zwo"
        filepath = os.path.join(output_dir, filename)
        
        # G√©n√©ration du contenu ZWO
        zwo_content = self._generate_zwo_content(workout_text, metadata)
        
        # √âcriture du fichier
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(zwo_content)
        
        logger.info(f"Fichier .zwo g√©n√©r√©: {filepath}")
        return filepath
    
    def _generate_zwo_content(self, workout_text: str, metadata: Dict[str, Any]) -> str:
        """G√©n√®re le contenu XML du fichier .zwo"""
        name = metadata.get('name', 'S√©ance Vekta')
        description = metadata.get('description', workout_text)
        duration = metadata.get('duration_minutes', 60)
        
        # Template ZWO basique
        zwo_template = f'''<?xml version="1.0" encoding="UTF-8"?>
<workout_file>
    <author>Vekta AI</author>
    <name>{name}</name>
    <description>{description}</description>
    <sportType>bike</sportType>
    <tags>
        <tag name="Vekta"/>
        <tag name="AI Generated"/>
    </tags>
    <workout>
        <!-- √âchauffement -->
        <Warmup Duration="600" PowerLow="0.5" PowerHigh="0.7"/>
        
        <!-- Travail principal -->
        <SteadyState Duration="{(duration-20)*60}" Power="0.85"/>
        
        <!-- Retour au calme -->
        <Cooldown Duration="600" PowerHigh="0.7" PowerLow="0.5"/>
    </workout>
</workout_file>'''
        
        return zwo_template 

# ================================
# G√âN√âRATION PARAM√âTRIQUE AVANC√âE
# ================================

@dataclass
class AdvancedWorkoutSegment:
    """Segment d'entra√Ænement param√©trique avanc√©"""
    name: str
    duration_minutes: int
    intensity_percent_start: int  # %FTP d√©but
    intensity_percent_end: int    # %FTP fin (pour ramps)
    zone: str
    description: str
    segment_type: str  # 'warmup', 'work', 'recovery', 'cooldown', 'ramp', 'steady'
    cadence_target: Optional[int] = None
    power_curve: Optional[str] = None  # 'linear', 'exponential', 'step'

@dataclass
class AdvancedParametricWorkout:
    """S√©ance d'entra√Ænement param√©trique compl√®te"""
    name: str
    description: str
    total_duration: int
    segments: List[AdvancedWorkoutSegment]
    difficulty: int  # 1-5
    workout_type: str
    training_load: float  # TSS estim√©
    target_systems: List[str]  # Syst√®mes √©nerg√©tiques cibl√©s
    periodization_phase: str  # 'base', 'build', 'peak', 'recovery'

class AdvancedParameterExtractor:
    """Extracteur de param√®tres avanc√© pour g√©n√©ration param√©trique production"""
    
    def __init__(self):
        # Zones d'intensit√© d√©taill√©es avec TSS
        self.intensity_zones = {
            'recuperation': {'power': (40, 55), 'zone': 'Zone 1', 'tss_factor': 0.3, 'systems': ['recovery']},
            'endurance': {'power': (56, 75), 'zone': 'Zone 2', 'tss_factor': 0.6, 'systems': ['aerobic']},
            'aerobic': {'power': (56, 75), 'zone': 'Zone 2', 'tss_factor': 0.6, 'systems': ['aerobic']},
            'tempo': {'power': (76, 90), 'zone': 'Zone 3', 'tss_factor': 0.8, 'systems': ['aerobic', 'threshold']},
            'sweet_spot': {'power': (84, 97), 'zone': 'Zone 3-4', 'tss_factor': 1.0, 'systems': ['threshold']},
            'seuil': {'power': (91, 105), 'zone': 'Zone 4', 'tss_factor': 1.2, 'systems': ['threshold', 'vo2']},
            'vo2max': {'power': (106, 120), 'zone': 'Zone 5', 'tss_factor': 1.5, 'systems': ['vo2', 'anaerobic']},
            'neuromuscular': {'power': (150, 300), 'zone': 'Zone 6', 'tss_factor': 0.5, 'systems': ['neuromuscular']}
        }
        
        # Structures d'entra√Ænement pr√©d√©finies
        self.workout_structures = {
            'pyramide': {'pattern': 'ascending_descending', 'recovery_ratio': 0.5},
            'ladder': {'pattern': 'ascending', 'recovery_ratio': 0.5},
            'over_under': {'pattern': 'alternating', 'recovery_ratio': 0.3},
            'micro_intervals': {'pattern': 'short_intervals', 'recovery_ratio': 1.0},
            'polarized': {'pattern': 'high_low', 'recovery_ratio': 2.0}
        }
        
        # Valeurs par d√©faut sophistiqu√©es
        self.defaults = {
            'warmup_progression': {'start': 50, 'end': 75, 'duration': 15},
            'cooldown_progression': {'start': 70, 'end': 50, 'duration': 10},
            'recovery_intensity': 55,
            'cadence_targets': {'endurance': 85, 'threshold': 95, 'vo2max': 105},
            'default_ftp': 250  # watts
        }
    
    def extract_advanced_parameters(self, corrected_query: str) -> Dict[str, Any]:
        """Extrait des param√®tres avanc√©s de la requ√™te"""
        query = corrected_query.lower()
        
        # Extraction compl√®te
        durations = self._extract_complex_durations(query)
        intensities = self._extract_detailed_intensities(query)
        structure = self._extract_advanced_structure(query)
        periodization = self._detect_periodization_phase(query)
        target_systems = self._identify_target_systems(query, intensities)
        
        return {
            'durations': durations,
            'intensities': intensities,
            'structure': structure,
            'periodization': periodization,
            'target_systems': target_systems,
            'workout_type': self._classify_advanced_workout_type(query, structure, intensities),
            'complexity_level': self._assess_complexity(query, structure),
            'has_warmup': self._has_warmup(query),
            'has_cooldown': self._has_cooldown(query)
        }
    
    def _extract_complex_durations(self, query: str) -> Dict[str, Any]:
        """Extraction avanc√©e des dur√©es avec patterns complexes"""
        durations = {
            'warmup': [],
            'work_blocks': [],
            'recovery_blocks': [],
            'cooldown': [],
            'total_work_time': 0,
            'work_recovery_ratio': 1.0
        }
        
        # Pattern pyramide: 1-2-3-2-1 min
        pyramid_match = re.search(r'pyramide.*?(\d+)-(\d+)-(\d+)-(\d+)-(\d+)', query)
        if pyramid_match:
            pyramid_durations = [int(x) for x in pyramid_match.groups()]
            durations['work_blocks'] = pyramid_durations
            durations['structure_type'] = 'pyramid'
        
        # Pattern standard: 5x3min
        standard_match = re.search(r'(\d+)\s*(?:x|fois)\s*(\d+)\s*(?:min|minute)', query)
        if standard_match:
            reps = int(standard_match.group(1))
            duration = int(standard_match.group(2))
            durations['work_blocks'] = [duration] * reps
            durations['structure_type'] = 'intervals'
        
        # Si aucune structure d√©tect√©e, par d√©faut continu
        if 'structure_type' not in durations:
            durations['structure_type'] = 'continuous'
        
        # Calcul du ratio travail/r√©cup√©ration
        work_time = sum(durations['work_blocks'])
        recovery_match = re.search(r'(\d+)\s*(?:min|minute)s?\s*(?:repos|recuperation|pause)', query)
        if recovery_match and work_time > 0:
            recovery_time = int(recovery_match.group(1))
            durations['work_recovery_ratio'] = work_time / recovery_time if recovery_time > 0 else 2.0
        
        return durations
    
    def _extract_detailed_intensities(self, query: str) -> Dict[str, Any]:
        """Extraction d√©taill√©e des intensit√©s avec zones sp√©cifiques"""
        intensities = {}
        
        # Pourcentages explicites
        percent_matches = re.findall(r'(\d+)\s*%(?:\s*(?:de\s*)?(?:ftp|pma|pc))?', query)
        if percent_matches:
            intensities['explicit_percentages'] = [int(p) for p in percent_matches]
        
        # Watts explicites
        watts_match = re.search(r'(\d+)\s*(?:watts?|w)', query)
        if watts_match:
            intensities['explicit_watts'] = int(watts_match.group(1))
        
        # Zones nomm√©es avec intensit√© variable
        for zone_name, zone_data in self.intensity_zones.items():
            if zone_name in query or any(alias in query for alias in self._get_zone_aliases(zone_name)):
                min_power, max_power = zone_data['power']
                intensities[zone_name] = {
                    'power_range': (min_power, max_power),
                    'zone': zone_data['zone'],
                    'target_power': (min_power + max_power) // 2,
                    'tss_factor': zone_data['tss_factor'],
                    'systems': zone_data['systems']
                }
        
        # Progression d'intensit√© (over-under, ramps)
        if 'over' in query and 'under' in query:
            intensities['pattern'] = 'over_under'
        elif 'progression' in query or 'ramp' in query:
            intensities['pattern'] = 'progressive'
        
        return intensities
    
    def _extract_advanced_structure(self, query: str) -> Dict[str, Any]:
        """Extraction de structures complexes d'entra√Ænement"""
        structure = {
            'type': 'continuous',
            'pattern': None,
            'complexity': 'simple',
            'micro_structure': None
        }
        
        # Structures nomm√©es
        for structure_name, structure_data in self.workout_structures.items():
            if structure_name in query:
                structure.update({
                    'type': structure_name,
                    'pattern': structure_data['pattern'],
                    'recovery_ratio': structure_data['recovery_ratio'],
                    'complexity': 'complex'
                })
                break
        
        # Micro-intervalles (30s on/30s off)
        micro_match = re.search(r'(\d+)s?\s*(?:on|travail).*?(\d+)s?\s*(?:off|repos)', query)
        if micro_match:
            structure['micro_structure'] = {
                'work_seconds': int(micro_match.group(1)),
                'recovery_seconds': int(micro_match.group(2)),
                'type': 'micro_intervals'
            }
        
        # D√©tection de complexit√©
        complexity_indicators = ['variation', 'progression', 'pyramide', 'over', 'under', 'micro']
        if any(indicator in query for indicator in complexity_indicators):
            structure['complexity'] = 'complex'
        
        return structure
    
    def _detect_periodization_phase(self, query: str) -> str:
        """D√©tecte la phase de p√©riodisation"""
        if any(term in query for term in ['base', 'fondamental', 'endurance']):
            return 'base'
        elif any(term in query for term in ['seuil', 'tempo', 'sweet', 'build']):
            return 'build'
        elif any(term in query for term in ['vo2', 'max', 'peak', 'pic']):
            return 'peak'
        elif any(term in query for term in ['recuperation', 'recovery', 'facile']):
            return 'recovery'
        else:
            return 'build'  # D√©faut
    
    def _identify_target_systems(self, query: str, intensities: Dict[str, Any]) -> List[str]:
        """Identifie les syst√®mes √©nerg√©tiques cibl√©s"""
        systems = set()
        
        for zone_data in intensities.values():
            if isinstance(zone_data, dict) and 'systems' in zone_data:
                systems.update(zone_data['systems'])
        
        # Ajouts bas√©s sur mots-cl√©s
        if 'endurance' in query:
            systems.add('aerobic')
        if 'sprint' in query or 'neuro' in query:
            systems.add('neuromuscular')
        if 'seuil' in query:
            systems.add('threshold')
        
        return list(systems) if systems else ['aerobic']
    
    def _classify_advanced_workout_type(self, query: str, structure: Dict[str, Any], intensities: Dict[str, Any]) -> str:
        """Classification avanc√©e du type de s√©ance"""
        # Priorisation bas√©e sur l'intensit√© principale
        if 'vo2max' in intensities:
            return 'VO2max Intervals'
        elif 'seuil' in intensities:
            return 'Threshold Training'
        elif 'sweet_spot' in intensities:
            return 'Sweet Spot'
        elif structure['type'] == 'pyramide':
            return 'Pyramid Workout'
        elif structure['complexity'] == 'complex':
            return 'Complex Training'
        elif 'endurance' in intensities:
            return 'Endurance Base'
        else:
            return 'Mixed Training'
    
    def _assess_complexity(self, query: str, structure: Dict[str, Any]) -> str:
        """√âvalue la complexit√© de la s√©ance"""
        complexity_score = 0
        
        # Facteurs de complexit√©
        if structure['type'] != 'continuous':
            complexity_score += 1
        if 'micro_structure' in structure and structure['micro_structure']:
            complexity_score += 1
        if len(re.findall(r'\d+', query)) > 5:  # Beaucoup de nombres
            complexity_score += 1
        if any(term in query for term in ['variation', 'progression', 'over', 'under']):
            complexity_score += 1
        
        if complexity_score == 0:
            return 'simple'
        elif complexity_score <= 2:
            return 'moderate'
        else:
            return 'complex'
    
    def _get_zone_aliases(self, zone_name: str) -> List[str]:
        """Retourne les alias pour une zone donn√©e"""
        aliases = {
            'recuperation': ['recup', 'recovery', 'facile'],
            'endurance': ['base', 'aerobic', 'fondamental'],
            'tempo': ['sweet', 'spot'],
            'seuil': ['threshold', 'lactate', 'lt'],
            'vo2max': ['max', 'fond', 'pma', 'vo2'],
            'neuromuscular': ['sprint', 'neuro', 'puissance']
        }
        return aliases.get(zone_name, [])
    
    def _has_warmup(self, query: str) -> bool:
        return bool(re.search(r'echauffement|chaude|chauffe|warm', query))
    
    def _has_cooldown(self, query: str) -> bool:
        return bool(re.search(r'retour|cool|calme', query))

class AdvancedParametricGenerator:
    """G√©n√©rateur param√©trique avanc√© pour production"""
    
    def __init__(self, extractor: AdvancedParameterExtractor):
        self.extractor = extractor
    
    def generate_advanced_workout(self, corrected_query: str, ftp_watts: int = 250) -> AdvancedParametricWorkout:
        """G√©n√®re une s√©ance param√©trique avanc√©e"""
        params = self.extractor.extract_advanced_parameters(corrected_query)
        
        segments = []
        total_duration = 0
        total_tss = 0.0
        
        # 1. √âchauffement sophistiqu√©
        if params['has_warmup']:
            warmup_segments = self._create_warmup_progression(params, ftp_watts)
            segments.extend(warmup_segments)
            total_duration += sum(s.duration_minutes for s in warmup_segments)
        
        # 2. Travail principal complexe
        main_segments = self._create_main_work(params, ftp_watts)
        segments.extend(main_segments)
        total_duration += sum(s.duration_minutes for s in main_segments)
        
        # 3. Retour au calme
        if params['has_cooldown']:
            cooldown_segments = self._create_cooldown_progression(params, ftp_watts)
            segments.extend(cooldown_segments)
            total_duration += sum(s.duration_minutes for s in cooldown_segments)
        
        # Calcul du TSS total
        total_tss = self._calculate_tss(segments, ftp_watts)
        
        # Nom sophistiqu√© de la s√©ance
        workout_name = self._generate_workout_name(params, total_duration, total_tss)
        
        return AdvancedParametricWorkout(
            name=workout_name,
            description=corrected_query,
            total_duration=total_duration,
            segments=segments,
            difficulty=self._calculate_advanced_difficulty(params, segments, total_tss),
            workout_type=params['workout_type'],
            training_load=total_tss,
            target_systems=params['target_systems'],
            periodization_phase=params['periodization']
        )
    
    def _create_warmup_progression(self, params: Dict[str, Any], ftp_watts: int) -> List[AdvancedWorkoutSegment]:
        """Cr√©e une progression d'√©chauffement sophistiqu√©e"""
        segments = []
        
        # √âchauffement de base (5 min √† 50%)
        segments.append(AdvancedWorkoutSegment(
            name="√âchauffement Base",
            duration_minutes=5,
            intensity_percent_start=50,
            intensity_percent_end=50,
            zone="Zone 1",
            description="Activation musculaire douce",
            segment_type="warmup",
            cadence_target=80
        ))
        
        # Progression (10 min de 60% √† 80%)
        segments.append(AdvancedWorkoutSegment(
            name="Progression d'√âchauffement",
            duration_minutes=10,
            intensity_percent_start=60,
            intensity_percent_end=80,
            zone="Zone 2",
            description="Mont√©e progressive en puissance",
            segment_type="ramp",
            cadence_target=85,
            power_curve="linear"
        ))
        
        # Pr√©paration sp√©cifique selon l'intensit√© cible
        if params['target_systems'] and 'vo2' in params['target_systems']:
            segments.append(AdvancedWorkoutSegment(
                name="Pr√©paration VO2",
                duration_minutes=3,
                intensity_percent_start=90,
                intensity_percent_end=90,
                zone="Zone 4",
                description="Pr√©paration sp√©cifique VO2max",
                segment_type="work",
                cadence_target=95
            ))
        
        return segments
    
    def _create_main_work(self, params: Dict[str, Any], ftp_watts: int) -> List[AdvancedWorkoutSegment]:
        """Cr√©e le travail principal complexe"""
        segments = []
        durations = params['durations']
        structure_type = durations.get('structure_type', 'continuous')
        
        if structure_type == 'pyramid':
            segments.extend(self._create_pyramid_work(params, ftp_watts))
        elif structure_type == 'intervals':
            segments.extend(self._create_interval_work(params, ftp_watts))
        else:
            segments.extend(self._create_continuous_work(params, ftp_watts))
        
        return segments
    
    def _create_pyramid_work(self, params: Dict[str, Any], ftp_watts: int) -> List[AdvancedWorkoutSegment]:
        """Cr√©e une s√©ance pyramide"""
        segments = []
        work_blocks = params['durations']['work_blocks']
        target_intensity = self._get_target_intensity(params['intensities'])
        
        for i, duration in enumerate(work_blocks):
            # Segment de travail
            segments.append(AdvancedWorkoutSegment(
                name=f"Pyramide {i+1}/{len(work_blocks)}",
                duration_minutes=duration,
                intensity_percent_start=target_intensity,
                intensity_percent_end=target_intensity,
                zone=self._intensity_to_zone(target_intensity),
                description=f"Bloc pyramide {duration}min",
                segment_type="work",
                cadence_target=self._get_cadence_for_intensity(target_intensity)
            ))
            
            # R√©cup√©ration (sauf dernier bloc)
            if i < len(work_blocks) - 1:
                recovery_duration = max(2, duration // 2)  # R√©cup = moiti√© du travail
                segments.append(AdvancedWorkoutSegment(
                    name=f"R√©cup√©ration {i+1}",
                    duration_minutes=recovery_duration,
                    intensity_percent_start=55,
                    intensity_percent_end=55,
                    zone="Zone 1",
                    description="R√©cup√©ration active",
                    segment_type="recovery",
                    cadence_target=80
                ))
        
        return segments
    
    def _create_interval_work(self, params: Dict[str, Any], ftp_watts: int) -> List[AdvancedWorkoutSegment]:
        """Cr√©e une s√©ance par intervalles standard"""
        segments = []
        work_blocks = params['durations']['work_blocks']
        target_intensity = self._get_target_intensity(params['intensities'])
        
        for i, duration in enumerate(work_blocks):
            # Segment de travail
            segments.append(AdvancedWorkoutSegment(
                name=f"Intervalle {i+1}/{len(work_blocks)}",
                duration_minutes=duration,
                intensity_percent_start=target_intensity,
                intensity_percent_end=target_intensity,
                zone=self._intensity_to_zone(target_intensity),
                description=f"Intervalle {duration}min √† {target_intensity}%FTP",
                segment_type="work",
                cadence_target=self._get_cadence_for_intensity(target_intensity)
            ))
            
            # R√©cup√©ration (sauf dernier intervalle)
            if i < len(work_blocks) - 1:
                recovery_duration = int(duration * params['durations'].get('work_recovery_ratio', 0.5))
                recovery_duration = max(1, recovery_duration)
                
                segments.append(AdvancedWorkoutSegment(
                    name=f"R√©cup√©ration {i+1}",
                    duration_minutes=recovery_duration,
                    intensity_percent_start=55,
                    intensity_percent_end=55,
                    zone="Zone 1",
                    description=f"R√©cup√©ration {recovery_duration}min",
                    segment_type="recovery",
                    cadence_target=80
                ))
        
        return segments
    
    def _create_continuous_work(self, params: Dict[str, Any], ftp_watts: int) -> List[AdvancedWorkoutSegment]:
        """Cr√©e un travail continu"""
        segments = []
        target_intensity = self._get_target_intensity(params['intensities'])
        duration = max(20, sum(params['durations']['work_blocks']) if params['durations']['work_blocks'] else 30)
        
        segments.append(AdvancedWorkoutSegment(
            name="Travail Continu",
            duration_minutes=duration,
            intensity_percent_start=target_intensity,
            intensity_percent_end=target_intensity,
            zone=self._intensity_to_zone(target_intensity),
            description=f"Travail continu {duration}min √† {target_intensity}%FTP",
            segment_type="work",
            cadence_target=self._get_cadence_for_intensity(target_intensity)
        ))
        
        return segments
    
    def _create_cooldown_progression(self, params: Dict[str, Any], ftp_watts: int) -> List[AdvancedWorkoutSegment]:
        """Cr√©e une progression de retour au calme"""
        segments = []
        
        # Retour progressif (10 min de 70% √† 50%)
        segments.append(AdvancedWorkoutSegment(
            name="Retour au Calme",
            duration_minutes=10,
            intensity_percent_start=70,
            intensity_percent_end=50,
            zone="Zone 1-2",
            description="Retour progressif au calme",
            segment_type="cooldown",
            cadence_target=75,
            power_curve="linear"
        ))
        
        return segments
    
    def _get_target_intensity(self, intensities: Dict[str, Any]) -> int:
        """D√©termine l'intensit√© cible principale"""
        if 'explicit_percentages' in intensities:
            return max(intensities['explicit_percentages'])
        elif intensities:
            # Prend l'intensit√© la plus √©lev√©e trouv√©e
            max_intensity = 0
            for zone_data in intensities.values():
                if isinstance(zone_data, dict) and 'target_power' in zone_data:
                    max_intensity = max(max_intensity, zone_data['target_power'])
            return max_intensity if max_intensity > 0 else 85
        else:
            return 85  # D√©faut
    
    def _intensity_to_zone(self, intensity_percent: int) -> str:
        """Convertit une intensit√© en zone d√©taill√©e"""
        if intensity_percent <= 55:
            return 'Zone 1 (R√©cup√©ration)'
        elif intensity_percent <= 75:
            return 'Zone 2 (Endurance)'
        elif intensity_percent <= 90:
            return 'Zone 3 (Tempo)'
        elif intensity_percent <= 105:
            return 'Zone 4 (Seuil)'
        elif intensity_percent <= 120:
            return 'Zone 5 (VO2max)'
        else:
            return 'Zone 6 (Neuromusculaire)'
    
    def _get_cadence_for_intensity(self, intensity_percent: int) -> int:
        """D√©termine la cadence optimale selon l'intensit√©"""
        if intensity_percent <= 75:
            return 85  # Endurance
        elif intensity_percent <= 90:
            return 90  # Tempo
        elif intensity_percent <= 105:
            return 95  # Seuil
        elif intensity_percent <= 120:
            return 105  # VO2max
        else:
            return 110  # Sprint
    
    def _calculate_tss(self, segments: List[AdvancedWorkoutSegment], ftp_watts: int) -> float:
        """Calcule le Training Stress Score total"""
        total_tss = 0.0
        
        for segment in segments:
            # Intensit√© moyenne du segment
            avg_intensity = (segment.intensity_percent_start + segment.intensity_percent_end) / 2
            intensity_factor = avg_intensity / 100
            
            # TSS = (Dur√©e en heures) √ó (IF^2) √ó 100
            duration_hours = segment.duration_minutes / 60
            segment_tss = duration_hours * (intensity_factor ** 2) * 100
            total_tss += segment_tss
        
        return round(total_tss, 1)
    
    def _calculate_advanced_difficulty(self, params: Dict[str, Any], segments: List[AdvancedWorkoutSegment], tss: float) -> int:
        """Calcule la difficult√© avanc√©e (1-5)"""
        # Facteurs de difficult√©
        max_intensity = max([s.intensity_percent_start for s in segments], default=50)
        complexity = params.get('complexity_level', 'simple')
        
        difficulty_score = 0
        
        # Bas√© sur l'intensit√© maximale
        if max_intensity >= 110:
            difficulty_score += 3
        elif max_intensity >= 95:
            difficulty_score += 2
        elif max_intensity >= 80:
            difficulty_score += 1
        
        # Bas√© sur le TSS
        if tss >= 100:
            difficulty_score += 2
        elif tss >= 60:
            difficulty_score += 1
        
        # Bas√© sur la complexit√©
        if complexity == 'complex':
            difficulty_score += 1
        
        # Conversion en √©chelle 1-5
        return min(5, max(1, difficulty_score))
    
    def _generate_workout_name(self, params: Dict[str, Any], duration: int, tss: float) -> str:
        """G√©n√®re un nom sophistiqu√© pour la s√©ance"""
        workout_type = params['workout_type']
        phase = params['periodization']
        
        # Nom bas√© sur le type et la phase
        base_name = f"{workout_type}"
        
        # Ajout de d√©tails
        if tss >= 100:
            intensity_label = "Intense"
        elif tss >= 60:
            intensity_label = "Mod√©r√©"
        else:
            intensity_label = "Facile"
        
        return f"{base_name} {intensity_label} - {duration}min (TSS: {tss})"

class AdvancedZwoGenerator(ZwoGenerator):
    """G√©n√©rateur .zwo avanc√© avec support de toutes les structures"""
    
    def create_advanced_zwo(self, workout: AdvancedParametricWorkout, ftp_watts: int = 250, output_dir: str = "./generated_workouts") -> str:
        """Cr√©e un fichier .zwo avanc√© avec structures complexes"""
        os.makedirs(output_dir, exist_ok=True)
        
        # Nom de fichier unique
        timestamp = int(time.time())
        filename = f"vekta_advanced_{timestamp}.zwo"
        filepath = os.path.join(output_dir, filename)
        
        # G√©n√©ration du contenu ZWO avanc√©
        zwo_content = self._generate_advanced_zwo_content(workout, ftp_watts)
        
        # √âcriture du fichier
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(zwo_content)
        
        logger.info(f"Fichier .zwo avanc√© g√©n√©r√©: {filepath}")
        return filepath
    
    def _generate_advanced_zwo_content(self, workout: AdvancedParametricWorkout, ftp_watts: int) -> str:
        """G√©n√®re le contenu XML avanc√© du fichier .zwo"""
        
        # Header XML avanc√©
        xml = '<?xml version="1.0" encoding="UTF-8"?>\n'
        xml += '<workout_file>\n'
        xml += f'    <author>Vekta AI Advanced</author>\n'
        xml += f'    <name>{workout.name}</name>\n'
        xml += f'    <description>{workout.description} | TSS: {workout.training_load} | Syst√®mes: {", ".join(workout.target_systems)}</description>\n'
        xml += '    <sportType>bike</sportType>\n'
        xml += '    <tags>\n'
        xml += '        <tag name="Vekta"/>\n'
        xml += '        <tag name="Advanced"/>\n'
        xml += '        <tag name="Parametric"/>\n'
        xml += f'        <tag name="{workout.workout_type}"/>\n'
        xml += f'        <tag name="{workout.periodization_phase.title()}"/>\n'
        xml += f'        <tag name="TSS{int(workout.training_load)}"/>\n'
        xml += '    </tags>\n'
        xml += '    <workout>\n'
        
        # Segments avanc√©s
        for segment in workout.segments:
            duration_seconds = segment.duration_minutes * 60
            power_start = segment.intensity_percent_start / 100.0
            power_end = segment.intensity_percent_end / 100.0
            
            if segment.segment_type == 'warmup':
                xml += f'        <Warmup Duration="{duration_seconds}" PowerLow="0.50" PowerHigh="{power_end:.2f}"/>\n'
            elif segment.segment_type == 'cooldown':
                xml += f'        <Cooldown Duration="{duration_seconds}" PowerHigh="{power_start:.2f}" PowerLow="0.50"/>\n'
            elif segment.segment_type == 'ramp':
                xml += f'        <Ramp Duration="{duration_seconds}" PowerLow="{power_start:.2f}" PowerHigh="{power_end:.2f}"/>\n'
            elif segment.segment_type in ['work', 'recovery']:
                if segment.cadence_target:
                    xml += f'        <SteadyState Duration="{duration_seconds}" Power="{power_start:.2f}" Cadence="{segment.cadence_target}"/>\n'
                else:
                    xml += f'        <SteadyState Duration="{duration_seconds}" Power="{power_start:.2f}"/>\n'
            
            # Ajout de commentaires pour les segments importants
            if segment.segment_type == 'work':
                xml += f'        <!-- {segment.description} - {segment.zone} -->\n'
        
        xml += '    </workout>\n'
        xml += '</workout_file>\n'
        
        return xml 